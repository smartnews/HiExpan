{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_transformers import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from pytorch_transformers import *\n",
    "from pytorch_transformers.modeling_bert import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_seed(dataset, file):\n",
    "    topic_words = {}\n",
    "    with open(dataset+'/result_'+file+'.txt') as f:\n",
    "        data=f.readlines()\n",
    "        current_topic = ''\n",
    "        for line in data:\n",
    "            if len(line.strip()) == 0:\n",
    "                current_topic = ''\n",
    "                continue\n",
    "            elif len(line.split(' ')) == 1:\n",
    "                current_topic = line.split(':')[0]\n",
    "                continue\n",
    "            elif current_topic != '':\n",
    "                topic_words[current_topic] = line.strip().split(' ')\n",
    "            # else:\n",
    "                # print(line)\n",
    "\n",
    "    return topic_words\n",
    "\n",
    "def get_emb(vec_file):\n",
    "    f = open(vec_file, 'r')\n",
    "    contents = f.readlines()[1:]\n",
    "    word_emb = {}\n",
    "    vocabulary = {}\n",
    "    vocabulary_inv = {}\n",
    "    emb_mat = []\n",
    "    for i, content in enumerate(contents):\n",
    "        content = content.strip()\n",
    "        tokens = content.split(' ')\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = [float(ele) for ele in vec]\n",
    "        word_emb[word] = np.array(vec)\n",
    "        vocabulary[word] = i\n",
    "        vocabulary_inv[i] = word\n",
    "        emb_mat.append(np.array(vec))\n",
    "    vocab_size = len(vocabulary)\n",
    "    emb_mat = np.array(emb_mat) \n",
    "    return word_emb, vocabulary, vocabulary_inv, emb_mat\n",
    "\n",
    "def get_temb(vec_file, topic_file):\n",
    "    topic2id = {}\n",
    "    topic_emb = {}\n",
    "    id2topic = {}\n",
    "    topic_hier = {}\n",
    "    i = 0\n",
    "    with open(topic_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parent = line.strip().split('\\t')[0]\n",
    "            temp = line.strip().split('\\t')[1]          \n",
    "            for topic in temp.split(' '):\n",
    "                topic2id[topic] = i\n",
    "                id2topic[i] = topic\n",
    "                i += 1\n",
    "                if parent not in topic_hier:\n",
    "                    topic_hier[parent] = []\n",
    "                topic_hier[parent].append(topic)\n",
    "    f = open(vec_file, 'r')\n",
    "    contents = f.readlines()[1:]\n",
    "    for i, content in enumerate(contents):\n",
    "        content = content.strip()\n",
    "        tokens = content.split(' ')\n",
    "        vec = tokens\n",
    "        vec = [float(ele) for ele in vec]\n",
    "        topic_emb[id2topic[i]] = np.array(vec)\n",
    "    return topic_emb, topic2id, id2topic, topic_hier\n",
    "\n",
    "def get_cap(vec_file, cap0_file=None):\n",
    "    print(vec_file)\n",
    "    f = open(vec_file, 'r')\n",
    "    contents = f.readlines()[1:]\n",
    "    word_cap = {}\n",
    "    for i, content in enumerate(contents):\n",
    "        content = content.strip()\n",
    "        tokens = content.split(' ')\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1]\n",
    "        vec = float(vec)\n",
    "        word_cap[word] = vec\n",
    "    \n",
    "    if cap0_file is not None:\n",
    "        with open(cap0_file) as f:\n",
    "            contents = f.readlines()[1:]\n",
    "            for i, content in enumerate(contents):\n",
    "                content = content.strip()\n",
    "                tokens = content.split(' ')\n",
    "                word = tokens[0]\n",
    "                vec = tokens[1]\n",
    "                vec = float(vec)\n",
    "                word_cap[word] = vec \n",
    "    return word_cap\n",
    "\n",
    "def topic_sim(query, idx2word, t_emb, w_emb):\n",
    "    if query in t_emb:\n",
    "        q_vec = t_emb[query]\n",
    "    else:\n",
    "        q_vec = w_emb[query]\n",
    "    word_emb = np.zeros((len(idx2word), 100))\n",
    "    for i in range(len(idx2word)):\n",
    "        word_emb[i] = w_emb[idx2word[i]]\n",
    "    res = np.dot(word_emb, q_vec)\n",
    "    res = res/np.linalg.norm(word_emb, axis=1)\n",
    "    sort_id = np.argsort(-res)\n",
    "\n",
    "    return sort_id\n",
    "\n",
    "def rank_cap(cap, idx2word, class_name):\n",
    "    word_cap = np.zeros(len(idx2word))\n",
    "    for i in range(len(idx2word)):\n",
    "        if idx2word[i] in cap:\n",
    "            word_cap[i] = (cap[idx2word[i]]-cap[class_name]) ** 2\n",
    "        else:\n",
    "            word_cap[i] = np.array([1.0])\n",
    "    low2high = np.argsort(word_cap)\n",
    "    return low2high\n",
    "\n",
    "def rank_cap_customed(cap, idx2word, class_idxs):\n",
    "    target_cap = np.mean([cap[idx2word[ind]] for ind in class_idxs])\n",
    "    word_cap = np.zeros(len(idx2word))\n",
    "#     print(f\"target capacity: {target_cap}\")\n",
    "    for i in range(len(idx2word)):\n",
    "        if idx2word[i] in cap:\n",
    "            word_cap[i] = (cap[idx2word[i]]-target_cap) ** 2\n",
    "#             tmp = cap[idx2word[i]]-target_cap\n",
    "#             if tmp > 0:\n",
    "#                 word_cap[i] = tmp\n",
    "#             else:\n",
    "#                 word_cap[i] = np.inf\n",
    "        else:\n",
    "            word_cap[i] = np.array([1.0])\n",
    "    low2high = np.argsort(word_cap)\n",
    "#     print(\"intermediate: \"+ str(word_cap[vocabulary['data_mining']]))\n",
    "#     print(low2high[0:5])\n",
    "    return low2high, target_cap\n",
    "\n",
    "def aggregate_ranking(sim, cap, word_cap, topic, idx2word, pretrain, target=None):\n",
    "    simrank2id = np.ones(len(sim)) * np.inf\n",
    "    caprank2id = np.ones(len(sim)) * np.inf\n",
    "    for i, w in enumerate(sim[:]):\n",
    "        simrank2id[w] = i + 1\n",
    "#     print(f'topic capacity:{word_cap[topic]}')\n",
    "#     print(f'target capcity: {target}')\n",
    "    for i, w in enumerate(cap):\n",
    "        if pretrain == 0:\n",
    "            if target is not None and word_cap[idx2word[w]] > target:\n",
    "                caprank2id[w] = i + 1\n",
    "            if target is None:\n",
    "                caprank2id[w] = i + 1\n",
    "    if pretrain == 0:        \n",
    "        agg_rank = simrank2id * caprank2id\n",
    "        final_rank = np.argsort(agg_rank)\n",
    "        final_rank_words = [idx2word[idx] for idx in final_rank[:500] if idx2word[idx] in ent_sent_index]\n",
    "    else:\n",
    "        agg_rank = simrank2id\n",
    "        final_rank = np.argsort(agg_rank)\n",
    "        final_rank_words = [idx2word[idx] for idx in final_rank[:500] if idx2word[idx] in ent_sent_index]\n",
    "#     print(f'\\n{topic} ranking list:')\n",
    "#     print([caprank2id[idx] for idx in sim[:20]])\n",
    "#     print([simrank2id[idx] for idx in final_rank[:20]])\n",
    "#     print([caprank2id[idx] for idx in final_rank[:20]])\n",
    "#     print(final_rank_words)\n",
    "#     f = open(out_file, 'a')\n",
    "#     f.write(f'\\n{topic}:\\n')\n",
    "#     f.write(' '.join(final_rank_words) + '\\n')    \n",
    "    return final_rank_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing training and test data\n",
    "# Basically use entity pairs to find their co-occurred sentences in the corpus\n",
    "# and then generate positive and negative training samples\n",
    "\n",
    "def process_training_data(rep_words, topic_hier, max_seq_length):\n",
    "\n",
    "    parent_list = [x for x in topic_hier if x != 'ROOT']\n",
    "    sentences_index = []\n",
    "    final_data = []\n",
    "\n",
    "    real_rep_words = {}\n",
    "    for key in rep_words:\n",
    "        real_rep_words[key] = []\n",
    "        \n",
    "    print(\"collecting positive samples!\")\n",
    "    \n",
    "    for parent in parent_list:\n",
    "        for child in topic_hier[parent]:\n",
    "#             print(child)\n",
    "            count = 10\n",
    "            for b in rep_words[child]:\n",
    "                if b not in ent_sent_index:\n",
    "                    continue\n",
    "                for a in rep_words[parent]:\n",
    "                    if a not in ent_sent_index:\n",
    "                        continue\n",
    "                    cooccur = ent_sent_index[a].intersection(ent_sent_index[b])\n",
    "                    if len(cooccur) > 0:\n",
    "                        if a not in real_rep_words[parent]:\n",
    "                            real_rep_words[parent].append(a)\n",
    "                        if b not in real_rep_words[child]:\n",
    "                            real_rep_words[child].append(b) \n",
    "                        print(a)\n",
    "                        print(b)\n",
    "                        for sen in cooccur:\n",
    "#                             if sen in sentences_index:\n",
    "#                                 continue\n",
    "                            sentences_index.append(sen)\n",
    "                            s = sentences[sen]\n",
    "                            s = '[CLS] '+s\n",
    "                            s = s.split(' ')\n",
    "                            if a not in s or b not in s:\n",
    "                                continue\n",
    "                            p_index = s.index(a)\n",
    "                            c_index = s.index(b)\n",
    "                            s[p_index] = '[MASK]'\n",
    "                            s[c_index] = '[MASK]'\n",
    "                            s = ' '.join(s).replace('_', ' ').replace('-lrb-','(').replace('-rrb-',')').split(' ')\n",
    "                            input_id = tokenizer.encode(' '.join(s))\n",
    "                            tokened_text = tokenizer.tokenize(' '.join(s))\n",
    "                            mask_id = [x for x in range(len(input_id)) if input_id[x]==103]  \n",
    "#                             if len(mask_id) < 2:\n",
    "#                                 print(' '.join(s))\n",
    "#                                 print(a)\n",
    "#                                 print(b)\n",
    "\n",
    "                            if len(input_id) > max_seq_length:\n",
    "                                if mask_id[1] - mask_id[0] >= max_seq_length:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    input_id = input_id[mask_id[0]:mask_id[1]+1]\n",
    "                                    p_index = 0 if p_index<c_index else mask_id[1] - mask_id[0]\n",
    "                                    c_index = 0 if c_index<p_index else mask_id[1] - mask_id[0]\n",
    "                            else:\n",
    "                                p_index = mask_id[0] if p_index<c_index else mask_id[1] \n",
    "                                c_index = mask_id[0] if c_index<p_index else mask_id[1]\n",
    "                            \n",
    "                            input_id = torch.tensor(input_id)\n",
    "                            r_sentence = F.pad(torch.tensor(input_id),(0,max_seq_length-len(input_id)), \"constant\", 0)\n",
    "                            attention_mask = torch.cat((torch.ones_like(input_id), torch.zeros(max_seq_length-len(input_id), dtype=torch.int64)),dim=0)\n",
    "                            p_mask = np.zeros((max_seq_length))\n",
    "                            p_mask[p_index] = 1\n",
    "                            c_mask = np.zeros((max_seq_length))\n",
    "                            c_mask[c_index] = 1\n",
    "                                            \n",
    "                            final_data.append([r_sentence, p_mask, c_mask, attention_mask, 0])\n",
    "                            final_data.append([r_sentence, c_mask, p_mask, attention_mask, 1])\n",
    "                            \n",
    "#                             if count > 0:\n",
    "#                                 print(a)\n",
    "#                                 print(b)\n",
    "#                                 print(r_sentence)\n",
    "#                                 print(tokened_text)\n",
    "#                                 print(p_index)\n",
    "#                                 print(p_mask)\n",
    "#                                 print(c_index)\n",
    "#                                 print(c_mask)\n",
    "#                                 print(attention_mask)\n",
    "#                                 print('\\n')\n",
    "#                                 count -= 1\n",
    "                                \n",
    "    pos_len = len(final_data)\n",
    "    print(f\"positive data number: {pos_len}\")\n",
    "                                \n",
    "    print(\"collecting negative samples from siblings!\")  \n",
    "    for parent in parent_list:\n",
    "        for child in topic_hier[parent]:\n",
    "#             print(child)\n",
    "            count = 10\n",
    "            for b in rep_words[child]:\n",
    "                if b not in ent_sent_index:\n",
    "                    continue\n",
    "                for a in rep_words[child]:\n",
    "                    if a == b:\n",
    "                        continue\n",
    "                    if a not in ent_sent_index:\n",
    "                        continue\n",
    "                    cooccur = ent_sent_index[a].intersection(ent_sent_index[b])\n",
    "                    if len(cooccur) > 0:\n",
    "                        for sen in cooccur:\n",
    "                            if sen in sentences_index:\n",
    "                                continue\n",
    "                            if np.random.random(1) > 0.1:\n",
    "                                continue\n",
    "#                             sentences_index.append(sen)\n",
    "                            s = sentences[sen]\n",
    "                            s = '[CLS] '+s\n",
    "                            s = s.split(' ')\n",
    "                            if a not in s or b not in s:\n",
    "                                continue\n",
    "                            p_index = s.index(a)\n",
    "                            c_index = s.index(b)\n",
    "                            s[p_index] = '[MASK]'\n",
    "                            s[c_index] = '[MASK]'\n",
    "                            s = ' '.join(s).replace('_', ' ').replace('-lrb-','(').replace('-rrb-',')').split(' ')\n",
    "                            input_id = tokenizer.encode(' '.join(s))\n",
    "                            mask_id = [x for x in range(len(input_id)) if input_id[x]==103]                                                       \n",
    "\n",
    "                            if len(input_id) > max_seq_length:\n",
    "                                if mask_id[1] - mask_id[0] >= max_seq_length:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    input_id = input_id[mask_id[0]:mask_id[1]+1]\n",
    "                                    p_index = 0 if p_index<c_index else mask_id[1] - mask_id[0]\n",
    "                                    c_index = 0 if c_index<p_index else mask_id[1] - mask_id[0]\n",
    "                            else:\n",
    "                                p_index = mask_id[0] if p_index<c_index else mask_id[1] \n",
    "                                c_index = mask_id[0] if c_index<p_index else mask_id[1]\n",
    "                            \n",
    "                            input_id = torch.tensor(input_id)\n",
    "                            r_sentence = F.pad(torch.tensor(input_id),(0,max_seq_length-len(input_id)), \"constant\", 0)\n",
    "                            attention_mask = torch.cat((torch.ones_like(input_id), torch.zeros(max_seq_length-len(input_id), dtype=torch.int64)),dim=0)\n",
    "                            p_mask = np.zeros((max_seq_length))\n",
    "                            p_mask[p_index] = 1\n",
    "                            c_mask = np.zeros((max_seq_length))\n",
    "                            c_mask[c_index] = 1\n",
    "                                            \n",
    "                            final_data.append([r_sentence, p_mask, c_mask, attention_mask, 2])\n",
    "                            \n",
    "\n",
    "    print(len(final_data))\n",
    "    \n",
    "    print(\"collecting negative samples from corpus!\")\n",
    "    while len(final_data) < pos_len * 2:\n",
    "#         if len(final_data) % 100 == 0:\n",
    "#             print(pos_len)\n",
    "#             print(len(final_data))\n",
    "        sen = np.random.choice(len(sentences))\n",
    "#         remove positive sentences\n",
    "        if sen in sentences_index:\n",
    "            continue\n",
    "            \n",
    "        s = sentences[sen]\n",
    "        s = '[CLS] '+s\n",
    "        s = sentences[sen].split(' ')\n",
    "        \n",
    "        #randomly choose pairs\n",
    "        entities = [x for x in s if \"_\" in x]\n",
    "        if len(entities) < 2:\n",
    "            continue\n",
    "        p_index, c_index = np.random.choice(len(entities),2)\n",
    "        while c_index == p_index:\n",
    "#             continue\n",
    "            c_index = np.random.choice(len(entities))\n",
    "            \n",
    "        s[p_index] = '[MASK]'\n",
    "        s[c_index] = '[MASK]'\n",
    "        s = ' '.join(s).replace('_', ' ').replace('-lrb-','(').replace('-rrb-',')').split(' ')\n",
    "        input_id = tokenizer.encode(' '.join(s))\n",
    "        mask_id = [x for x in range(len(input_id)) if input_id[x]==103]                                                       \n",
    "\n",
    "        if len(input_id) > max_seq_length:\n",
    "            if mask_id[1] - mask_id[0] >= max_seq_length:\n",
    "                continue\n",
    "            else:\n",
    "                input_id = input_id[mask_id[0]:mask_id[1]+1]\n",
    "                p_index = 0 if p_index<c_index else mask_id[1] - mask_id[0]\n",
    "                c_index = 0 if c_index<p_index else mask_id[1] - mask_id[0]\n",
    "        else:\n",
    "            p_index = mask_id[0] if p_index<c_index else mask_id[1] \n",
    "            c_index = mask_id[0] if c_index<p_index else mask_id[1]\n",
    "\n",
    "        input_id = torch.tensor(input_id)\n",
    "        r_sentence = F.pad(torch.tensor(input_id),(0,max_seq_length-len(input_id)), \"constant\", 0)\n",
    "        attention_mask = torch.cat((torch.ones_like(input_id), torch.zeros(max_seq_length-len(input_id), dtype=torch.int64)),dim=0)\n",
    "\n",
    "        p_mask = np.zeros((max_seq_length))\n",
    "        p_mask[p_index] = 1\n",
    "        c_mask = np.zeros((max_seq_length))\n",
    "        c_mask[c_index] = 1\n",
    "\n",
    "        final_data.append([r_sentence, p_mask, c_mask, attention_mask, 2])\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    return final_data, sentences_index\n",
    "\n",
    "\n",
    "def process_test_data(test_topic_rep_words, test_cand, max_seq_length):\n",
    "\n",
    "    print(\"collecting positive samples!\")\n",
    "    final_data = []\n",
    "    \n",
    "\n",
    "    count = 10\n",
    "    for b in test_topic_rep_words:\n",
    "        if b not in ent_sent_index or b not in ename2embed_bert:\n",
    "            continue\n",
    "        for a in test_cand:\n",
    "            if a not in ent_sent_index or a not in ename2embed_bert:\n",
    "                continue\n",
    "            if a == b:\n",
    "                continue\n",
    "            cooccur = ent_sent_index[a].intersection(ent_sent_index[b])\n",
    "            if len(cooccur) > 0:\n",
    "                for sen in cooccur:\n",
    "#                     if sen in sentences_index:\n",
    "#                         continue\n",
    "#                     sentences_index.append(sen)\n",
    "                    s = sentences[sen]\n",
    "                    s = '[CLS] '+s\n",
    "                    s = s.split(' ')\n",
    "                    if a not in s or b not in s:\n",
    "                        continue\n",
    "                    p_index = s.index(a)\n",
    "                    c_index = s.index(b)\n",
    "                    s[p_index] = '[MASK]'\n",
    "                    s[c_index] = '[MASK]'\n",
    "                    s = ' '.join(s).replace('_', ' ').replace('-lrb-','(').replace('-rrb-',')').split(' ')\n",
    "                    input_id = tokenizer.encode(' '.join(s))\n",
    "                    mask_id = [x for x in range(len(input_id)) if input_id[x]==103]\n",
    "#                     if len(mask_id) < 2:\n",
    "#                         print(sentences[sen])\n",
    "#                         print(a)\n",
    "#                         print(b)\n",
    "\n",
    "                    if len(input_id) > max_seq_length:\n",
    "                        if mask_id[1] - mask_id[0] >= max_seq_length:\n",
    "                            continue\n",
    "                        else:\n",
    "                            input_id = input_id[mask_id[0]:mask_id[1]+1]\n",
    "                            p_index = 0 if p_index<c_index else mask_id[1] - mask_id[0]\n",
    "                            c_index = 0 if c_index<p_index else mask_id[1] - mask_id[0]\n",
    "                    else:\n",
    "                        p_index = mask_id[0] if p_index<c_index else mask_id[1] \n",
    "                        c_index = mask_id[0] if c_index<p_index else mask_id[1]\n",
    "\n",
    "                    input_id = torch.tensor(input_id)\n",
    "                    r_sentence = F.pad(torch.tensor(input_id),(0,max_seq_length-len(input_id)), \"constant\", 0)\n",
    "                    attention_mask = torch.cat((torch.ones_like(input_id), torch.zeros(max_seq_length-len(input_id), dtype=torch.int64)),dim=0)\n",
    "\n",
    "                    p_mask = np.zeros((max_seq_length))\n",
    "                    p_mask[p_index] = 1\n",
    "                    c_mask = np.zeros((max_seq_length))\n",
    "                    c_mask[c_index] = 1\n",
    "\n",
    "                    final_data.append([r_sentence, p_mask, c_mask, attention_mask, a])\n",
    "                    final_data.append([r_sentence, c_mask, p_mask, attention_mask, a])\n",
    "                    \n",
    "\n",
    "#                     if count > 0:\n",
    "#                         print(r_sentence)\n",
    "#                         print(p_index)\n",
    "#                         print(c_index)\n",
    "#                         print(attention_mask)\n",
    "#                         print('\\n')\n",
    "#                         count -= 1\n",
    "\n",
    "    \n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def generate_batch(batch):\n",
    "    input_ids = torch.tensor([np.array(entry[0]) for entry in batch])\n",
    "    entity1_mask = torch.tensor([entry[1] for entry in batch])\n",
    "    entity2_mask = torch.tensor([entry[2] for entry in batch])\n",
    "    attention_mask = torch.tensor([np.array(entry[3]) for entry in batch])\n",
    "    labels = torch.tensor([entry[4] for entry in batch])\n",
    "    return input_ids, entity1_mask, entity2_mask, attention_mask, labels \n",
    "\n",
    "def generate_test_batch(batch):\n",
    "    input_ids = torch.tensor([np.array(entry[0]) for entry in batch])\n",
    "    entity1_mask = torch.tensor([entry[1] for entry in batch])\n",
    "    entity2_mask = torch.tensor([entry[2] for entry in batch])\n",
    "    attention_mask = torch.tensor([np.array(entry[3]) for entry in batch])\n",
    "    entity = [entry[4] for entry in batch]\n",
    "    return input_ids, entity1_mask, entity2_mask, attention_mask, entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/validation/testing function of Bert model\n",
    "def train_func(sub_train_, BATCH_SIZE, optimizer, scheduler):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    for i, (input_ids, entity1_mask, entity2_mask, attention_mask, labels) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, entity1_mask, entity2_mask, attention_mask, labels = input_ids.to(device), entity1_mask.float().to(device), entity2_mask.float().to(device), attention_mask.to(device), labels.to(device)\n",
    "        output, loss = model(input_ids, attention_mask=attention_mask, entity1_mask=entity1_mask, entity2_mask=entity2_mask, labels=labels)\n",
    "        train_acc += (output.argmax(1) == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss/len(sub_train_), train_acc/len(sub_train_)\n",
    "\n",
    "def valid_func(data_):\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for i, (input_ids, entity1_mask, entity2_mask, attention_mask, labels) in enumerate(data):\n",
    "        with torch.no_grad():\n",
    "            input_ids, entity1_mask, entity2_mask, attention_mask, labels = input_ids.to(device), entity1_mask.float().to(device), entity2_mask.float().to(device), attention_mask.to(device), labels.to(device)\n",
    "            output, loss = model(input_ids, attention_mask=attention_mask, entity1_mask=entity1_mask, entity2_mask=entity2_mask, labels=labels)\n",
    "            valid_acc += (output.argmax(1) == labels).sum().item()\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "\n",
    "    return valid_loss / len(data_), valid_acc/len(data_)\n",
    "\n",
    "def test(data_, BATCH_SIZE):\n",
    "    logits = []\n",
    "    entities = []\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_test_batch)\n",
    "    for i, (input_ids, entity1_mask, entity2_mask, attention_mask, entity) in enumerate(data):\n",
    "        entities.extend(entity)\n",
    "        input_ids, entity1_mask, entity2_mask, attention_mask = input_ids.to(device), entity1_mask.float().to(device), entity2_mask.float().to(device), attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask=attention_mask, entity1_mask=entity1_mask, entity2_mask=entity2_mask)\n",
    "            logits.extend(F.softmax(output, dim=1).cpu().numpy())\n",
    "    return logits, entities \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel + Linear Classifier\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import *\n",
    "from pytorch_transformers.modeling_bert import *\n",
    "\n",
    "class RelationClassifier(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RelationClassifier, self).__init__(config)\n",
    "#         super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        token_type_ids=None, \n",
    "        attention_mask=None, \n",
    "        entity1_mask=None, \n",
    "        entity2_mask=None, \n",
    "        labels=None):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            encoded_layers = self.bert(input_ids, attention_mask)[0]\n",
    "            batch_size, max_seq_length = entity1_mask.shape[0],entity1_mask.shape[1]\n",
    "\n",
    "            diag_entity1_mask_ = []\n",
    "            for i in range(batch_size):\n",
    "                diag_entity1_mask_.append(torch.diag(entity1_mask[i]).cpu().numpy())\n",
    "            diag_entity1_mask = torch.tensor(diag_entity1_mask_).cuda()\n",
    "\n",
    "            diag_entity2_mask_ = []\n",
    "            for i in range(batch_size):\n",
    "                diag_entity2_mask_.append(torch.diag(entity2_mask[i]).cpu().numpy())\n",
    "            diag_entity2_mask = torch.tensor(diag_entity2_mask_).cuda()\n",
    "\n",
    "            # Concatenate two entity embedding      \n",
    "            batch_entity1_emb = torch.matmul(diag_entity1_mask, encoded_layers).permute(0,2,1)\n",
    "            batch_entity2_emb = torch.matmul(diag_entity2_mask, encoded_layers).permute(0,2,1)\n",
    "            batch_entity_emb = torch.cat((batch_entity1_emb, batch_entity2_emb), dim=1)\n",
    "\n",
    "\n",
    "            pooling = nn.MaxPool1d(kernel_size=max_seq_length, stride=1)\n",
    "            entity_emb_output = pooling( batch_entity_emb ).squeeze()\n",
    "            entity_emb_output = self.dropout(entity_emb_output)\n",
    "        \n",
    "        # Linear layer classifier\n",
    "        logits = self.classifier(entity_emb_output)\n",
    "\n",
    "\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return logits , loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p,q):\n",
    "    d = 0\n",
    "    num_labels = len(p)\n",
    "    for i in range(num_labels):\n",
    "        d += p[i] * np.log(p[i]/q[i])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def type_consistent(topic_word_dict0, ename2embed_bert, print_cls = False):\n",
    "    topic_word_dict = {}\n",
    "    all_words = []\n",
    "\n",
    "    for topic in topic_word_dict0:\n",
    "        topic_word_dict[topic] = []\n",
    "        for ename in topic_word_dict0[topic]:\n",
    "#             ename = ename.replace('_',' ')\n",
    "            if ename in ename2embed_bert:\n",
    "                topic_word_dict[topic].append(ename)\n",
    "                all_words.append(ename)\n",
    "                \n",
    "    topics = list(topic_word_dict0.keys())\n",
    "\n",
    "    all_words.extend([x for x in topics if x in ename2embed_bert])\n",
    "\n",
    "    # all_words.extend(['POTENTIAL_PARENT_'+x for x in potential_parents if x in ename2embed_bert])\n",
    "    # all_embed.extend([ename2embed_bert[x][0] for x in potential_parents if x in ename2embed_bert])\n",
    "\n",
    "    all_children = []\n",
    "    all_embed = []\n",
    "    all_words_and_their_parents = []   \n",
    "    for word in all_words:\n",
    "        topic_count = 0\n",
    "        word0 = word\n",
    "        for topic in topic_word_dict:\n",
    "            if word in topic_word_dict[topic]:\n",
    "                word0 = '('+topic+')'+word0\n",
    "                topic_count += 1\n",
    "        if topic_count > 1:\n",
    "            continue\n",
    "        all_words_and_their_parents.append(word0)\n",
    "        all_children.append(word)\n",
    "        all_embed.append(ename2embed_bert[word][0])\n",
    "\n",
    "    # AP\n",
    "    clustering = AffinityPropagation().fit(all_embed)\n",
    "    n_clusters = max(clustering.labels_) + 1\n",
    "    clusters = {}\n",
    "    singular_words = []\n",
    "    for i in range(n_clusters):\n",
    "        clusters[i] = [ all_words_and_their_parents[x] for x in range(len(clustering.labels_)) if clustering.labels_[x]==i]\n",
    "        if print_cls:\n",
    "            print(clusters[i])\n",
    "        category_count = set()\n",
    "        for word0 in clusters[i]:\n",
    "            tmp = word0.split('(')\n",
    "            for seg in tmp:\n",
    "                tmp2 = seg.split(')')\n",
    "                if len(tmp2) < 2:\n",
    "                    continue\n",
    "                category_count.add(tmp2[0])\n",
    "            if len(category_count) > 1:\n",
    "                break\n",
    "#         print(len(category_count))\n",
    "        if len(category_count) <= 1:\n",
    "            singular_words.extend([all_words[x] for x in range(len(clustering.labels_)) if clustering.labels_[x]==i])\n",
    "\n",
    "#     print(singular_words)\n",
    "\n",
    "    new_topic_word_dict = {}\n",
    "    for topic in topic_word_dict:\n",
    "        new_topic_word_dict[topic] = []\n",
    "        for ename in topic_word_dict[topic]:\n",
    "            if ename not in singular_words:\n",
    "#                 ename = ename.replace(' ','_')\n",
    "                new_topic_word_dict[topic].append(ename)\n",
    "\n",
    "    return new_topic_word_dict\n",
    "\n",
    "def type_consistent_for_list(l, topic_word_dict0, ename2embed_bert, print_cls = False):\n",
    "    if len(l) < 60:\n",
    "        return l\n",
    "    tmp_cluster = {}\n",
    "    tmp_cluster['0'] = l\n",
    "    for topic in topic_word_dict0:\n",
    "        tmp_cluster[topic] = topic_word_dict0[topic]\n",
    "    tmp_cluster = type_consistent(tmp_cluster, ename2embed_bert, print_cls)    \n",
    "#     print(tmp_cluster['0'])\n",
    "    l = tmp_cluster['0']\n",
    "    return l\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_inference(test_data, BATCH_SIZE,mode='child'):\n",
    "    logits, cand_entities = test(test_data, BATCH_SIZE)\n",
    "    \n",
    "    if(len(test_data)==0):\n",
    "        return {},{}\n",
    "\n",
    "    logits = np.array(logits)\n",
    "    labels = np.argmax(logits, axis=1)\n",
    "    test_num = len(labels)\n",
    "    entity_ratio = {}\n",
    "    entity_count = {}\n",
    "\n",
    "    for i in range(int(test_num/2)):\n",
    "        ent = cand_entities[2*i]\n",
    "        l1 = labels[2*i]\n",
    "        kl1 = kl_divergence([1/3,1/3,1/3],logits[2*i])\n",
    "        l2 = labels[2*i + 1]\n",
    "        kl2 = kl_divergence([1/3,1/3,1/3],logits[2*i+1])\n",
    "        if ent not in entity_ratio:\n",
    "            entity_ratio[ent] = np.zeros((3))\n",
    "        entity_ratio[ent] += logits[2*i]\n",
    "        entity_ratio[ent] += [logits[2*i+1][1], logits[2*i+1][0], logits[2*i+1][2]]\n",
    "#         print(f'{cand_entities[2*i]} {logits[2*i]} {kl1} {kl2}')\n",
    "        if kl1 > 0.5 and kl2 > 0.5:\n",
    "            if mode == 'child':\n",
    "                if l1 == 0 and l2 == 1 and logits[2*i][0]>0.7 and logits[2*i+1][1]>0.7:\n",
    "#                     print(f'parent: {cand_entities[2*i]}')\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][0] += 1\n",
    "                elif l1 == 1 and l2 == 0 and logits[2*i][1]>0.7 and logits[2*i+1][0]>0.7:\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][1] += 1\n",
    "#                     print(f'child: {cand_entities[2*i]} {logits[2*i]} {kl1} {kl2}')\n",
    "                elif l1 == 2 and l2 == 2:\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][2] += 1\n",
    "            elif mode == 'parent':\n",
    "                if l1 == 0 and l2 == 1 :\n",
    "        #             print(f'parent: {cand_entities[2*i]}')\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][0] += 1\n",
    "                elif l1 == 1 and l2 == 0 :\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][1] += 1\n",
    "    #                 print(f'child: {cand_entities[2*i]} {logits[2*i]} {kl1} {kl2}')\n",
    "                elif l1 == 2 and l2 == 2:\n",
    "                    if ent not in entity_count:\n",
    "                        entity_count[ent] = np.zeros((3))\n",
    "                    entity_count[ent][2] += 1\n",
    "    #             print(f'no relation: {cand_entities[2*i]}')\n",
    "    #         print(f\"kl1: {kl1} kl2: {kl2}\")\n",
    "    return entity_ratio, entity_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_all_rel(test_topics, entity_count_alltopics, mode='child'):\n",
    "    child_entities_count = defaultdict(list)\n",
    "    entity_confidence = defaultdict(float)\n",
    "    for test_topic in test_topics:\n",
    "#         print(f\"test topic: {test_topic}\")\n",
    "        entity_count = entity_count_alltopics[test_topic]\n",
    "\n",
    "        for ent in entity_count:            \n",
    "            ratio = entity_count[ent]/np.sum(entity_count[ent])\n",
    "#             print(f\"{ent}: parent: {ratio[0]} child: {ratio[1]} no relation: {ratio[2]}\")\n",
    "            if mode == 'child' and ratio[1] > 0.7:\n",
    "                child_entities_count[test_topic].append(ent)\n",
    "                entity_confidence[ent] += ratio[1]\n",
    "            elif mode == 'parent' and ratio[0] > 0.7:\n",
    "                child_entities_count[test_topic].append(ent)\n",
    "                entity_confidence[ent] += ratio[0]\n",
    "    for ent in entity_confidence:\n",
    "        entity_confidence[ent] /= len(test_topics)\n",
    "    entity_conf = sorted(entity_confidence.items(), key = lambda x: x[1], reverse = True)\n",
    "#     print(entity_conf)\n",
    "    return child_entities_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_common_ent_for_list(l):\n",
    "\n",
    "    parent_cand = set()\n",
    "\n",
    "    for test_topic in l:\n",
    "        if len(parent_cand) == 0:\n",
    "            parent_cand = ent_ent_index[test_topic]\n",
    "        else:\n",
    "            parent_cand = parent_cand.intersection(ent_ent_index[test_topic])\n",
    "\n",
    "    return parent_cand\n",
    "\n",
    "def get_common_ent_for_list_with_dict(l,d):\n",
    "    parent_result = set()\n",
    "    for test_topic in l:\n",
    "        if len(parent_result) == 0:\n",
    "            parent_result = set(d[test_topic])\n",
    "        else:\n",
    "            parent_result = parent_result.intersection(set(d[test_topic]))\n",
    "\n",
    "    return parent_result\n",
    "\n",
    "def get_threshold_from_dict(d, thre):\n",
    "    parent_result_entities = defaultdict(int)\n",
    "    for topic in d:\n",
    "        for ent in d[topic]:\n",
    "            parent_result_entities[ent] += 1\n",
    "#     print(parent_result_entities)\n",
    "    parent_result_entities = [x for x in parent_result_entities if parent_result_entities[x] >= len(d)*thre]\n",
    "#     print(parent_result_entities)\n",
    "    return parent_result_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster by type, and delete \n",
    "\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "def type_consistent_cocluster(topic_word_dict0, ename2embed_bert, n_cluster_min, print_cls = False, save_file=None):\n",
    "    topic_word_dict = {}\n",
    "    all_words = []\n",
    "\n",
    "    for topic in topic_word_dict0:\n",
    "        topic_word_dict[topic] = []\n",
    "        for ename in topic_word_dict0[topic]:\n",
    "            if ename in ename2embed_bert:\n",
    "                topic_word_dict[topic].append(ename)\n",
    "                all_words.append(ename)\n",
    "                \n",
    "    topics = list(topic_word_dict0.keys())\n",
    "#     print(\"topics\")\n",
    "#     print(topics)\n",
    "\n",
    "    all_children = [x for x in all_words]\n",
    "#     all_words.extend([x for x in topics if x in ename2embed_bert])\n",
    "    all_embed = [ename2embed_bert[x][0] for x in all_words]\n",
    "#     print(all_children)\n",
    "\n",
    "\n",
    "    all_words_and_their_parents = []\n",
    "    for word in all_words:\n",
    "        for topic in topic_word_dict:\n",
    "            if word in topic_word_dict[topic]:\n",
    "                word0 = (topic, word)\n",
    "                break\n",
    "        all_words_and_their_parents.append(word0)\n",
    "#     print(all_words_and_their_parents)\n",
    "        \n",
    "\n",
    "    # AP\n",
    "    clustering = AffinityPropagation().fit(all_embed)\n",
    "    n_clusters = max(clustering.labels_) + 1\n",
    "    clusters = {}\n",
    "    col_vectors = np.zeros( (len(topic_word_dict) ,n_clusters), dtype=float)\n",
    "    type_dict = {}\n",
    "    for i in range(n_clusters):\n",
    "        clusters[i] = [ all_words_and_their_parents[x] for x in range(len(clustering.labels_)) if clustering.labels_[x]==i]\n",
    "        if print_cls:\n",
    "            print(i)\n",
    "            print(clusters[i])\n",
    "            for ind, w in clusters[i]:\n",
    "                type_dict[w] = str(i)\n",
    "        for word0 in clusters[i]:\n",
    "            word0_col = int(word0[0])\n",
    "            col_vectors[word0_col,i] = 1\n",
    "    col_vectors = np.array(col_vectors)\n",
    "    col_vectors += 0.1*np.ones( (len(topic_word_dict) ,n_clusters), dtype=int)\n",
    "    \n",
    "    \n",
    "    for n_cluster in range(n_cluster_min, n_cluster_min+10):\n",
    "    \n",
    "        model = SpectralCoclustering(n_clusters=n_cluster, random_state=0)\n",
    "        model.fit(col_vectors)\n",
    "\n",
    "        new_topic_word_dict = {}\n",
    "#         print('row cluster result:', model.row_labels_)\n",
    "#         print('col cluster result:', model.column_labels_)\n",
    "        coverage_list = []\n",
    "        for ind in range(n_cluster):\n",
    "#             print(ind)\n",
    "            small_matrix = col_vectors[[x for x in range(len(model.row_labels_)) if model.row_labels_[x] == ind]]\n",
    "            small_matrix = small_matrix[:,[x for x in range(len(model.column_labels_)) if model.column_labels_[x] == ind]]\n",
    "            coverage_list.append(np.sum(small_matrix)/np.sum(np.ones_like(small_matrix)))\n",
    "        if max(coverage_list) >= 0.7:\n",
    "            break\n",
    "            \n",
    "    fit_data = col_vectors[np.argsort(model.row_labels_)]\n",
    "    fit_data = fit_data[:, np.argsort(model.column_labels_)]\n",
    "    \n",
    "    cluster_count = [sum(model.row_labels_==x) for x in range(n_cluster)]\n",
    "#     print(\"row cluster count: \", cluster_count)\n",
    "    \n",
    "    cluster_count = [sum(model.column_labels_==x) for x in range(n_cluster)]\n",
    "#     print(\"column cluster count: \", cluster_count)\n",
    "\n",
    "    \n",
    "    coverage_thre = min(max(coverage_list), 0.4)\n",
    "#     print('coverage: ',coverage_list)\n",
    "    \n",
    "    for ind in range(n_cluster):\n",
    "        if coverage_list[ind] <= coverage_thre:\n",
    "#             print(\"del cluster \",ind)\n",
    "            continue\n",
    "        for topic in topic_word_dict:\n",
    "            if model.row_labels_[int(topic)] == ind:\n",
    "                new_topic_word_dict[topic] = [x for x in topic_word_dict[topic]]\n",
    "\n",
    "    if print_cls:\n",
    "        pass\n",
    "#         if clustering.labels_[int(topic)]==0:\n",
    "#             new_topic_word_dict[topic] = [x for x in topic_word_dict[topic]]\n",
    "#         else:\n",
    "#             print('del ',topic_word_dict[topic])\n",
    "\n",
    "    return new_topic_word_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select corpus\n",
    "dataset = 'yelp'\n",
    "file='des'\n",
    "topic_file='topics_des.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus!\n",
      "finish loading corpus!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ent_sent_index.txt: record the sentence id where each entity occurs; used for generating BERT training sample\n",
    "print('loading corpus!')\n",
    "ent_sent_index = dict()\n",
    "with open(dataset+'/ent_sent_index.txt') as f:\n",
    "    for line in f:\n",
    "        ent = line.split('\\t')[0]\n",
    "        tmp = line.strip().split('\\t')[1].split(' ')\n",
    "        tmp = [int(x) for x in tmp]\n",
    "        ent_sent_index[ent] = set(tmp)\n",
    "\n",
    "# sentences_.txt: sentence id to text\n",
    "sentences = dict()\n",
    "with open(dataset+'/sentences_.txt') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        sentences[i] = line\n",
    "        \n",
    "ent_ent_index = dict()\n",
    "with open(dataset+'/ent_ent_index.txt') as f:\n",
    "    for line in f:\n",
    "        ent = line.split('\\t')[0]\n",
    "        tmp = line.strip().split('\\t')[1].split(' ')\n",
    "        ent_ent_index[ent] = set(tmp)\n",
    "        \n",
    "\n",
    "# ename2embed_bert = loadEnameEmbedding(os.path.join(dataset, 'BERTembed_5.txt'), 768)\n",
    "# # ename2embed_bert = loadEnameEmbedding(os.path.join(dataset, 'bert_word_emb.txt'), 768)\n",
    "\n",
    "\n",
    "print('finish loading corpus!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp/emb_part_des_cap.txt\n",
      "seafood\n",
      "burger\n",
      "salad\n",
      "dessert\n",
      "ice_cream\n",
      "cake\n",
      "pastries\n"
     ]
    }
   ],
   "source": [
    "pretrain=0\n",
    "# load word embedding\n",
    "word_emb, vocabulary, vocabulary_inv, emb_mat = get_emb(vec_file=os.path.join(dataset, 'emb_part_'+file + '_w.txt'))\n",
    "\n",
    "# load topic embedding\n",
    "topic_emb, topic2id, id2topic, topic_hier = get_temb(vec_file=os.path.join(dataset, 'emb_part_'+file+'_t.txt'), topic_file=os.path.join(dataset, topic_file))\n",
    "\n",
    "# load word specificity\n",
    "word_cap = get_cap(vec_file=os.path.join(dataset, 'emb_part_'+file+'_cap.txt'))\n",
    "\n",
    "# load bert embedding\n",
    "ename2embed_bert = loadEnameEmbedding(os.path.join(dataset, 'BERTembed.txt'), 768)\n",
    "\n",
    "# calculate topic representative words: rep_words\n",
    "rep_words = {}\n",
    "for topic in topic_emb:\n",
    "    print(topic)\n",
    "    sim_ranking = topic_sim(topic, vocabulary_inv, topic_emb, word_emb)\n",
    "    if pretrain:\n",
    "        cap_ranking = np.ones((len(vocabulary)))\n",
    "        word_cap1 = np.ones((len(vocabulary)))\n",
    "    else:\n",
    "        cap_ranking = rank_cap(word_cap, vocabulary_inv, topic)    \n",
    "    rep_words[topic] = aggregate_ranking(sim_ranking, cap_ranking, word_cap, topic, vocabulary_inv, pretrain)\n",
    "rep_words1 = {}\n",
    "for topic in topic_emb:\n",
    "    rep_words1[topic] = [x for x in rep_words[topic]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seafood': ['seafood'], 'burger': ['burger'], 'salad': ['salad'], 'dessert': ['dessert'], 'ice_cream': ['ice_cream'], 'cake': ['cake'], 'pastries': ['pastries']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in rep_words:\n",
    "    rep_words[word] = [word]\n",
    "print(rep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Relation Statement Classifier\n",
    "\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_transformers import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "BATCH_SIZE=16\n",
    "TEST_BATCH_SIZE=512\n",
    "EPOCHS = 5\n",
    "max_seq_length = 128\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification(config)\n",
    "# model = BertModel(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RelationClassifier.from_pretrained('bert-base-uncased')\n",
    "model.float()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting positive samples!\n",
      "dessert\n",
      "ice_cream\n",
      "dessert\n",
      "cake\n",
      "dessert\n",
      "pastries\n",
      "positive data number: 2650\n",
      "collecting negative samples from siblings!\n",
      "2650\n",
      "collecting negative samples from corpus!\n"
     ]
    }
   ],
   "source": [
    "# generating training data\n",
    "total_data, sentences_index = process_training_data(rep_words, topic_hier, max_seq_length)\n",
    "train_data = total_data[:int(len(total_data)/2*0.95)]\n",
    "train_data.extend(total_data[int(len(total_data)/2):int(len(total_data)/2+len(total_data)/2*0.95)])\n",
    "valid_data = total_data[int(len(total_data)/2*0.95):int(len(total_data)/2)]\n",
    "valid_data.extend(total_data[int(len(total_data)/2*0.95+len(total_data)/2):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data point number: 5034\n"
     ]
    }
   ],
   "source": [
    "print(f\"training data point number: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.0163(train)\t|\tAcc: 90.4%(train)\n",
      "\tLoss: 0.0123(valid)\t|\tAcc: 91.4%(valid)\n",
      "Epoch: 1  | time in 0 minutes, 45 seconds\n",
      "\tLoss: 0.0091(train)\t|\tAcc: 94.6%(train)\n",
      "\tLoss: 0.0094(valid)\t|\tAcc: 94.4%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 45 seconds\n",
      "\tLoss: 0.0076(train)\t|\tAcc: 95.8%(train)\n",
      "\tLoss: 0.0094(valid)\t|\tAcc: 93.2%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 45 seconds\n",
      "\tLoss: 0.0070(train)\t|\tAcc: 96.1%(train)\n",
      "\tLoss: 0.0092(valid)\t|\tAcc: 92.9%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 45 seconds\n",
      "\tLoss: 0.0068(train)\t|\tAcc: 96.2%(train)\n",
      "\tLoss: 0.0091(valid)\t|\tAcc: 93.6%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 45 seconds\n"
     ]
    }
   ],
   "source": [
    "# training the bert classifier\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_data, BATCH_SIZE, optimizer, scheduler)\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    \n",
    "    valid_loss, valid_acc = valid_func(valid_data)\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    \n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: seafood\n",
      "collecting positive samples!\n",
      "test data point number: 6076\n",
      "topic: burger\n",
      "collecting positive samples!\n",
      "test data point number: 16956\n",
      "topic: salad\n",
      "collecting positive samples!\n",
      "test data point number: 49954\n",
      "topic: dessert\n",
      "collecting positive samples!\n",
      "test data point number: 21254\n"
     ]
    }
   ],
   "source": [
    "# Depth Expansion: get the relation classification result of test data\n",
    "# train_topic='dessert'\n",
    "entity_ratio_alltopics = {}\n",
    "entity_count_alltopics = {}\n",
    "\n",
    "training_topic = {}\n",
    "for topic in topic_hier['ROOT']:\n",
    "    if topic in topic_hier:\n",
    "        training_topic[topic] = word_cap[topic]\n",
    "training_topic = sorted(training_topic.items(), key = lambda x: x[1])\n",
    "train_topic = training_topic[0][0]\n",
    "\n",
    "for test_topic in topic_hier['ROOT']:\n",
    "    \n",
    "    sim_ranking = topic_sim(test_topic, vocabulary_inv, topic_emb, word_emb)\n",
    "    cap_ranking, target_cap = rank_cap_customed(word_cap, vocabulary_inv, [vocabulary[word] for word in topic_hier[train_topic]])\n",
    "    coefficient = max(word_cap[test_topic] / word_cap[train_topic],1)\n",
    "    test_cand = aggregate_ranking(sim_ranking, cap_ranking, word_cap, test_topic, vocabulary_inv, pretrain, target_cap*coefficient)\n",
    "    print(f'topic: {test_topic}')\n",
    "    test_data = process_test_data(rep_words[test_topic], test_cand, max_seq_length)\n",
    "    print(f\"test data point number: {len(test_data)}\")\n",
    "#     if len(test_data) > 10000:\n",
    "#         test_data = test_data[:10000]\n",
    "\n",
    "    entity_ratio, entity_count = relation_inference(test_data, TEST_BATCH_SIZE)\n",
    "    entity_ratio_alltopics[test_topic] = entity_ratio\n",
    "    entity_count_alltopics[test_topic] = entity_count\n",
    "\n",
    "child_entities_count = sum_all_rel(topic_hier['ROOT'], entity_count_alltopics, mode='child')\n",
    "\n",
    "# print(child_entities_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " seafood\n",
      "['king_crab', 'shellfish', 'shrimps', 'oyster', 'snow_crab_legs', 'mussel', 'crabs', 'lobsters', 'crab_cakes', 'fresh_oysters', 'raw_oysters', 'crab_claws', 'rockefeller', 'chowder', 'oyster_bar', 'crabmeat', 'maine', 'shell', 'shells', 'snow', 'seafood_section', 'smoked_salmon', 'fried_oysters', 'stone_crab', 'peel', 'jumbo', 'snails', 'alligator', 'gazpacho']\n",
      "\n",
      "['squid', 'ocean', 'wasabi', 'roe', 'miso', 'ponzu', 'spicy_sauce', 'marinade', 'nori']\n",
      "\n",
      "['sea_bass', 'fishes', 'snapper', 'cod', 'toro', 'tuna_tartar', 'quail', 'seared', 'mackerel', 'sea_urchin', 'kobe_beef', 'carpaccio', 'hamachi', 'butterfish', 'grilled_squid', 'diver', 'sea_scallops', 'freshness', 'ahi_tuna', 'courses', 'seared_tuna', 'fin']\n",
      "\n",
      "['crawfish', 'dungeness_crab', 'catfish', 'grits', 'etouffee', 'sampler', 'pan', 'sausages']\n",
      "\n",
      "['fish_balls', 'saut_ed', 'pork_chops', 'veg', 'snow_peas', 'carrots', 'bean_sprouts', 'poultry']\n",
      "\n",
      "['skewers', 'kimchi', 'bulgogi', 'galbi', 'fusion', 'overly_salty']\n",
      "\n",
      "['sashimi', 'imitation_crab', 'seaweed', 'spicy_tuna', 'teriyaki', 'edamame', 'dynamite', 'dragon_roll', 'spicy_salmon']\n",
      "\n",
      "['stir_fried', 'shumai', 'rangoon', 'steam', 'orange_chicken', 'potstickers', 'chow_fun', 'kung_pao_chicken', 'fried_fish', 'beef_chow_fun', 'bbq_pork', 'sticky_rice', 'steamed_rice', 'malaysian', 'firecracker', 'fried_dumplings', 'authentic_chinese']\n",
      "\n",
      "\n",
      " burger\n",
      "['hotdog', 'cheesesteak', 'hot_dogs', 'buns', 'coney', 'corn_dog', 'sonoran']\n",
      "\n",
      "['tots', 'regular_fries', 'smash_fries', 'animal_style', 'onion_straws', 'big_mac', 'skinny_fries', 'potato_chips', 'pickle', 'shoestring', 'shoestring_fries', 'curds', 'honey_mustard', 'pimento_cheese', 'loaded_fries', 'fry_sauce', 'tenders', 'fires', 'strips', 'burger_patties', 'pretzel', 'potato_wedges', 'thousand_island', 'texas_toast', 'crinkle_cut_fries', 'nuggets', 'strings', 'fried_mushrooms', 'homemade_chips', 'barbecue', 'garlic_parmesan', 'naked', 'reg', 'heinz', 'fried_green_tomato']\n",
      "\n",
      "['caramelized_onions', 'cheddar', 'fried_egg', 'bleu_cheese', 'carmelized_onions', 'cheddar_cheese', 'smash', 'american_cheese', 'swiss', 'swiss_cheese', 'applewood_smoked_bacon', 'smoked_bacon', 'crispy_bacon', 'bacon_jam', 'brie', 'gruyere_cheese', 'portobello', 'grilled_cheese_sandwiches', 'rocket', 'popper', 'cowboy', 'gruyere', 'maple_syrup', 'gouda', 'sauteed_onions', 'smoked_gouda', 'earl', 'shrooms', 'piled_high', 'buttermilk', 'tomato_jam', 'stack', 'dijon']\n",
      "\n",
      "['aioli', 'pepper_jack', 'charred', 'hamburger_patty', 'fried_onions', 'wheat_bun', 'smothered', 'char', 'fixings', 'dripping', 'smashed', 'saut_ed_onions', 'ground_beef', 'thousand_island_dressing', 'wedges', 'au_jus', 'sauteed_mushrooms', 'mayonnaise', 'toasted_bun', 'perfectly_seasoned', 'runny', 'cooked_correctly', 'caramelized_onion']\n",
      "\n",
      "['pepper_jack_cheese', 'jalapenos', 'jalape_os', 'coke', 'jalapeno', 'condiments', 'relish', 'jalepenos', 'chorizo', 'sriracha', 'sour_cream', 'nacho_cheese', 'green_chiles', 'hatch', 'lb', 'fully_loaded']\n",
      "\n",
      "['oreo', 'frozen_custard']\n",
      "\n",
      "['parmesan', 'provolone', 'mozzarella', 'gorgonzola', 'pepperoni']\n",
      "\n",
      "['truffle', 'frites', 'brussel_sprouts', 'brussels_sprouts', 'ahi_tuna', 'quinoa']\n",
      "\n",
      "['pastrami', 'mustard', 'corned_beef', 'smoked_meat', 'kraut', 'sauerkraut', 'pig']\n",
      "\n",
      "\n",
      " salad\n",
      "['lettuce', 'cucumber', 'cucumbers', 'carrots', 'cilantro']\n",
      "\n",
      "['squash', 'lentils', 'butternut_squash', 'peas', 'pea']\n",
      "\n",
      "['beets', 'pine_nuts', 'pear', 'beet', 'pears', 'walnuts', 'grapes', 'brie', 'almonds', 'candied_walnuts', 'tart', 'cranberries', 'pumpkin_seeds', 'candied_pecans', 'pecans', 'raisins']\n",
      "\n",
      "['lemon', 'ginger', 'orange']\n",
      "\n",
      "['quinoa', 'greens', 'spring_mix', 'avocado', 'couscous', 'crisp', 'shredded', 'fresh_greens', 'citrus', 'brown', 'market', 'roasted_vegetables', 'bistro', 'sprouts']\n",
      "\n",
      "['spinach', 'feta', 'olives', 'mozzarella', 'gorgonzola', 'kalamata', 'artichokes', 'parmesan', 'prosciutto', 'fennel', 'fresh_mozzarella', 'pancetta', 'roasted_red_peppers', 'basil_pesto', 'hearts', 'asiago', 'roasted_tomatoes', 'ricotta']\n",
      "\n",
      "['tomatoes', 'onions', 'mushrooms', 'red_onion', 'peppers', 'red_peppers', 'chickpeas', 'pepper', 'bell_pepper', 'bean_sprouts']\n",
      "\n",
      "['corn', 'black_beans']\n",
      "\n",
      "['blue_cheese', 'celery', 'wedge', 'dill', 'green_beans', 'breast', 'blue_cheese_crumbles', 'pickles', 'ranch_dressing', 'portobello', 'ranch', 'bleu_cheese', 'aioli', 'banana_peppers', 'brussels_sprouts']\n",
      "\n",
      "['balsamic', 'parsley', 'olive_oil', 'anchovies', 'herb', 'olive', 'oil', 'balsamic_vinegar', 'herbs', 'parmesan_cheese']\n",
      "\n",
      "['chili']\n",
      "\n",
      "\n",
      " dessert\n",
      "['cheesecake', 'creme_brulee', 'mousse', 'chocolate_cake', 'impressive', 'bread_pudding', 'tiramisu', 'creme_brule', 'flourless_chocolate_cake', 'panna_cotta', 'key_lime_pie', 'flan', 'lava', 'chocolate_souffle', 'lemon_tart', 'profiteroles', 'budino', 'polenta', 'champagne', 'affogato', 'fondue', 'tres_leches_cake', 'brown_butter', 'mains', 'sharing', 'napoleon', 'tres_leches', 'fried_banana', 'rose', 'mouse']\n",
      "\n",
      "['caramel', 'pudding', 'dark_chocolate', 'pistachio', 'peanut_butter', 'salted_caramel', 'white_chocolate', 'toffee', 'almond', 'key_lime', 'chocolate_brownie', 'butterscotch', 'red_velvet', 'ganache', 'strawberry_shortcake', 'sherbet', 'meringue', 'pumpkin', 'red_velvet_cake', 'pecan', 'praline', 'coconut_cream_pie', 'dulce_de_leche', 'fudge', 'pecan_pie', 'overly_sweet', 'jelly', 'mascarpone', 'strudel', 'red_velvet_cupcake', 'nut', 'rocky_road', 'glaze', 'buttercream', 'carmel', 'chestnut', 'mint_chocolate_chip', 'graham_cracker', 'amaretto', 'pumpkin_pie', 'cocoa', 'chantilly']\n",
      "\n",
      "['banana', 'strawberries', 'carrot', 'spinach', 'bananas', 'fresh_fruit', 'berries', 'fruits', 'cherries', 'pecans', 'raspberries', 'maple_syrup', 'almonds', 'compote']\n",
      "\n",
      "['french_toast', 'toast', 'pancakes', 'pancake']\n",
      "\n",
      "['gelato', 'cookies', 'cakes', 'brownies', 'tarts', 'crepes', 'pastries', 'cannoli', 'macaroon', 'treats', 'bakery', 'eclairs', 'candy', 'truffles', 'chocolate_chip_cookies', 'woman', 'biscotti']\n",
      "\n",
      "['donuts', 'donut', 'doughnut', 'doughnuts']\n",
      "\n",
      "['hot_chocolate', 'latte', 'cappuccino', 'mocha', 'chai']\n",
      "\n",
      "['soda', 'cups', 'beverages']\n",
      "\n",
      "['pastry', 'chocolate_chip', 'caf', 'baked_goods', 'muffin', 'muffins', 'danish', 'scone', 'oatmeal', 'raisin', 'quiche']\n",
      "\n",
      "['strawberry', 'sorbet', 'raspberry', 'lemon', 'apple', 'tart', 'blueberry', 'berry', 'cinnamon', 'peach', 'ginger', 'seasonal', 'pear', 'martini', 'rhubarb', 'pineapple', 'candied', 'blackberry', 'apricot', 'orange', 'lavender', 'honey', 'pairing', 'bourbon', 'refreshing', 'fig']\n",
      "\n",
      "['brownie', 'cookie', 'nutella', 'flat', 'sundae', 'oreo', 'smores', 'waffle', 'cotton_candy', 'brownie_sundae', 'center', 'milkshake', 'churro', 'banana_pudding', 'peanut_butter_cup', 'chain', 'cones', 'scoops', 'oreo_cookie', 'funnel_cake', 'turtle', 'sweet_potato', 'boy']\n",
      "\n",
      "['vanilla', 'crepe', 'whipped_cream', 'topping', 'scoop', 'syrup', 'marshmallow', 'cone', 'icing', 'birthday_cake', 'bag', 'cereal', 'ube', 'hot_fudge', 'whip_cream', 'sprinkles', 'frozen_yogurt', 'chocolate_chips', 'chocolate_syrup', 'waffle_cone', 'powdered_sugar']\n",
      "\n",
      "['flavours', 'rice_pudding', 'rush', 'yogurt', 'baklava', 'jello', 'sticky_rice', 'japanese', 'platter', 'gulab_jamun', 'assorted', 'foods', 'curry', 'tofu', 'balls']\n",
      "\n",
      "['black', 'corn', 'bean']\n",
      "\n",
      "['ice', 'milk', 'green_tea', 'mochi', 'soft_serve', 'matcha', 'shaved_ice', 'sesame', 'lychee', 'taro', 'tapioca']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "child_entities = type_consistent(child_entities_count, ename2embed_bert)\n",
    "# child_entities=child_entities_count\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "clusters_all = {}\n",
    "k=0\n",
    "start_list = [0]\n",
    "for j,topic in enumerate(topic_hier['ROOT']): \n",
    "    if topic not in child_entities:\n",
    "        continue\n",
    "    X = []\n",
    "    \n",
    "    for ent in child_entities[topic]:\n",
    "        if ent not in word_emb:\n",
    "            continue\n",
    "        X.append(word_emb[ent])\n",
    "    X = np.array(X)\n",
    "#     print(X)\n",
    "#     if len(X) == 0:\n",
    "#         print(len(X))\n",
    "#         continue\n",
    "    clustering = AffinityPropagation().fit(X)\n",
    "    n_clusters = max(clustering.labels_) + 1\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        clusters[str(i)] = [child_entities[topic][x] for x in range(len(clustering.labels_)) if clustering.labels_[x] == i]\n",
    "        \n",
    "        clusters_all[str(k)] = clusters[str(i)]\n",
    "        k+=1\n",
    "    start_list.append(k)\n",
    "#     new_clusters = type_consistent_col(clusters, ename2embed_bert)\n",
    "    \n",
    "#     i=0\n",
    "#     for k in new_clusters: \n",
    "# #         print(clusters[k])\n",
    "#         if len(new_clusters[k])>1:\n",
    "#             print(i,':', end='\\t')\n",
    "#             print(new_clusters[k])\n",
    "#             i += 1\n",
    "# print(clusters_all)    \n",
    "# new_clusters = type_consistent_col(clusters_all, ename2embed_bert)\n",
    "# print(clusters_all)\n",
    "new_clusters = type_consistent_cocluster(clusters_all, ename2embed_bert, n_cluster_min = 2, print_cls=False)\n",
    "\n",
    "# print(start_list)\n",
    "\n",
    "tmp = defaultdict(list)\n",
    "\n",
    "topic_idx = 0\n",
    "for k in range(len(clusters_all)):\n",
    "    if k >= start_list[topic_idx]:\n",
    "        print('\\n',topic_hier['ROOT'][topic_idx])\n",
    "        topic_idx += 1\n",
    "    if str(k) in new_clusters:# and len(new_clusters[str(k)]) > 1:\n",
    "        print(new_clusters[str(k)])\n",
    "        print('')\n",
    "        tmp[topic_hier['ROOT'][topic_idx-1]].append(new_clusters[str(k)])\n",
    "\n",
    "child_entities = tmp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "['flavors', 'replacement', 'app', 'dosa', 'carne_asada', 'southwestern', 'pan', 'main_ingredient', 'bun', 'flatbread', 'portion_sizes', 'sprouts', 'cookie', 'dates', 'dipping_sauces', 'cheeseburger', 'appetizer', 'burrito', 'deserts', 'lunch_special', 'cocktail', 'combo', 'sampler', 'sake', 'roast', 'treats', 'goods', 'coconut', 'hundreds', 'soy', 'stated', 'summer', 'typical', 'charcuterie', 'shops', 'thinking', 'watering', 'ground', 'curious', 'skimp', 'fried_egg', 'seasonal', 'adam', 'spinach', 'called', 'burgers', 'bonus', 'amount', 'herb', 'hair', 'vinegar', 'essence', 'none', 'olive', 'scotch_egg', 'guests', 'chicken_wings', 'parmigiana', 'depending', 'hype', '100', 'locations', 'pretty_solid', 'thursday_night', 'fact', 'chopped_salad', 'hook', 'husband_loves', 'beans', 'sweet_sour', 'american', 'days', 'smoked_salmon', 'yonge', 'lola', 'event', 'sun', 'dreams', 'tangy', 'member', 'venetian', 'border', 'art', 'double', 'fly', 'description', 'leftovers', 'snacks', 'sour_cream', 'carb', 'patty', 'roll', 'weeks_ago', 'ramen', 'separate', 'sticky', 'promise', 'skin', 'crepes', 'jasmine', 'entr_es', 'fairly_large', 'tamales', 'pizzas', \"n't_recall\", 'including_tip', 'bigger', \"n't_hurt\", 'bottomless', 'assortment', 'domestic', 'sweeter', 'lasagna', 'martini', 'cooked_correctly', 'bag', 'pistachios', 'beef_cheek', 'cross', 'eve', '70', 'chile', 'tacos', 'complimentary', 'price_tag', 'salad_bar', 'freshly_baked', 'soaked', 'stuff', 'minute', 'drooling', 'nigiri', 'coffee', 'olive_oil', 'fusion', 'nicer', '60', 'center', 'tang', 'problem', 'favorite_item', 'sushi_roll', '2', 'paella', 'br', 'buffet', 'reasonable_prices', 'g', 'dollar', 'wraps', 'rotten', 'street', 'google', 'line', 'mexican_food', 'photo', 'recipes', 'dripping', 'mini', 'variety', 'friendly', 'generous', 'kobe_beef', 'fried_fish', 'bbq_chicken', 'rude', 'kim', 'poor_quality', 'n', 'nacho', 'carrot', 'rubbery', 'fairly_quickly', 'destination', 'sandwich', 'exhausted', 'excuse', 'true', 'onion', 'ahi', 'fried_tofu', 'accident', 'weekly', 'arancini', 'mozzarella_sticks', 'mmm', 'crunch', 'ambiance', 'moments', 'sliders', 'wondering', 'dry', 'barbecue', 'sunday', 'plating', 'consume', 'generous_portion', 'fresh', 'boneless', 'vegetarians', 'issue', 'york', 'lost', 'brussel_sprouts', 'muffin', 'wontons', 'char', 'panini', 'personal_favourite', 'french', 'company', 'passing', 'tax', 'family_members', 'mushroom', 'hummus', 'food_court']\n"
     ]
    }
   ],
   "source": [
    "# Root Node Candidate Generation!\n",
    "\n",
    "\n",
    "parent_cand = get_common_ent_for_list(topic_hier['ROOT'])\n",
    "if len(parent_cand) > 1000:\n",
    "    parent_cand = type_consistent_for_list(parent_cand, rep_words, ename2embed_bert, False)\n",
    "print(len(parent_cand))\n",
    "print(parent_cand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test topic: seafood\n",
      "collecting positive samples!\n",
      "test data point number: 4996\n",
      "test topic: burger\n",
      "collecting positive samples!\n",
      "test data point number: 12986\n",
      "test topic: salad\n",
      "collecting positive samples!\n",
      "test data point number: 17558\n",
      "test topic: dessert\n",
      "collecting positive samples!\n",
      "test data point number: 7048\n",
      "['southwestern', 'lunch_special', 'roast', 'seasonal', 'guests', 'carb', 'entr_es', 'salad_bar', 'sushi_roll', 'buffet', 'recipes', 'fried_tofu', 'ambiance', 'plating', 'generous_portion', 'vegetarians', 'carne_asada', 'summer', 'lola', 'dreams', 'leftovers', 'ramen', 'barbecue']\n"
     ]
    }
   ],
   "source": [
    "parent_entity_ratio_alltopics = {}\n",
    "parent_entity_count_alltopics = {}\n",
    "from random import sample\n",
    "\n",
    "\n",
    "for test_topic in topic_hier['ROOT']:\n",
    "    print(f'test topic: {test_topic}')\n",
    "    \n",
    "    test_data = process_test_data([test_topic], list(parent_cand), max_seq_length)\n",
    "    print(f\"test data point number: {len(test_data)}\")\n",
    "    \n",
    "#     if len(test_data) > 10000:\n",
    "#         test_data = sample(test_data, 10000)\n",
    "    \n",
    "    \n",
    "    entity_ratio, entity_count = relation_inference(test_data, TEST_BATCH_SIZE,mode='child')\n",
    "    parent_entity_ratio_alltopics[test_topic] = entity_ratio\n",
    "    parent_entity_count_alltopics[test_topic] = entity_count\n",
    "\n",
    "parent_entities_count = sum_all_rel(topic_hier['ROOT'], parent_entity_count_alltopics, mode='parent')\n",
    "parent_result = get_threshold_from_dict(parent_entities_count, 1/2)\n",
    "# print(len(parent_result))\n",
    "\n",
    "parent_result = type_consistent_for_list(parent_result, rep_words, ename2embed_bert, False)\n",
    "print(parent_result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test topic: southwestern\n",
      "collecting positive samples!\n",
      "test data point number: 876\n",
      "test topic: lunch_special\n",
      "collecting positive samples!\n",
      "test data point number: 5572\n",
      "test topic: roast\n",
      "collecting positive samples!\n",
      "test data point number: 1968\n",
      "test topic: seasonal\n",
      "collecting positive samples!\n",
      "test data point number: 3550\n",
      "test topic: guests\n",
      "collecting positive samples!\n",
      "test data point number: 9922\n",
      "test topic: carb\n",
      "collecting positive samples!\n",
      "test data point number: 1138\n",
      "test topic: entr_es\n",
      "collecting positive samples!\n",
      "test data point number: 986\n",
      "test topic: salad_bar\n",
      "collecting positive samples!\n",
      "test data point number: 5778\n",
      "test topic: sushi_roll\n",
      "collecting positive samples!\n",
      "test data point number: 1302\n",
      "test topic: buffet\n",
      "collecting positive samples!\n",
      "test data point number: 32112\n",
      "test topic: recipes\n",
      "collecting positive samples!\n",
      "test data point number: 2180\n",
      "test topic: fried_tofu\n",
      "collecting positive samples!\n",
      "test data point number: 958\n",
      "test topic: ambiance\n",
      "collecting positive samples!\n",
      "test data point number: 19402\n",
      "test topic: plating\n",
      "collecting positive samples!\n",
      "test data point number: 990\n",
      "test topic: generous_portion\n",
      "collecting positive samples!\n",
      "test data point number: 2040\n",
      "test topic: vegetarians\n",
      "collecting positive samples!\n",
      "test data point number: 1900\n",
      "test topic: carne_asada\n",
      "collecting positive samples!\n",
      "test data point number: 7104\n",
      "test topic: summer\n",
      "collecting positive samples!\n",
      "test data point number: 9918\n",
      "test topic: lola\n",
      "collecting positive samples!\n",
      "test data point number: 932\n",
      "test topic: dreams\n",
      "collecting positive samples!\n",
      "test data point number: 956\n",
      "test topic: leftovers\n",
      "collecting positive samples!\n",
      "test data point number: 6070\n",
      "test topic: ramen\n",
      "collecting positive samples!\n",
      "test data point number: 21222\n",
      "test topic: barbecue\n",
      "collecting positive samples!\n",
      "test data point number: 2852\n",
      "[('eggs', 1.210843), ('stuffed', 1.099042), ('creamy', 1.134547), ('flavor', 0.827247), ('red', 1.071105), ('fries', 1.041714), ('salsa', 1.18477), ('pieces', 1.079834), ('cheese', 0.973151), ('chicken', 0.921372), ('gravy', 1.248186), ('bacon', 1.070453), ('burger', 1.057532), ('seafood', 1.072297), ('sauce', 0.881957), ('chips', 1.131858), ('spicy', 0.998534), ('steak', 0.959556), ('green', 1.091455), ('soup', 1.086947), ('side', 0.826599), ('sweet', 0.868755), ('beef', 0.995621), ('pasta', 1.11281), ('rolls', 1.125227), ('sandwich', 1.0138), ('huge', 0.870109), ('cut', 1.006843), ('salad', 0.942083), ('fish', 1.043067), ('noodles', 1.117401), ('pizza', 1.038159), ('extra', 0.958727), ('coffee', 1.030409), ('bread', 1.010474), ('hot', 0.943537), ('bowl', 1.043033), ('fresh', 0.839513), ('white', 1.046609), ('shrimp', 1.040965), ('cake', 1.115259), ('veggies', 1.042153), ('fried_rice', 1.214096), ('butter', 1.124733), ('potatoes', 1.099702), ('crab', 1.236949), ('tea', 1.078188), ('meat', 0.849512), ('beans', 1.20265), ('garlic', 1.087274), ('pork', 1.056521), ('house', 0.846993), ('sides', 1.093369), ('dry', 0.989003), ('plate', 0.906362), ('vegetables', 1.088573), ('rice', 1.043745), ('vegetable', 1.118238), ('salmon', 1.121971), ('right', 0.776929), ('sauces', 1.10953), ('toppings', 1.132025), ('wine', 1.133241), ('spice', 1.154941)]\n",
      "['red', 'fries', 'cheese', 'bacon', 'burger', 'spicy', 'steak', 'beef', 'sandwich', 'cut', 'fish', 'pizza', 'extra', 'coffee', 'bread', 'hot', 'bowl', 'white', 'shrimp', 'veggies', 'pork', 'dry', 'rice']\n",
      "['red', 'fries', 'cheese', 'bacon', 'burger', 'spicy', 'steak', 'beef', 'sandwich', 'cut', 'fish', 'pizza', 'extra', 'coffee', 'bread', 'hot', 'bowl', 'white', 'shrimp', 'veggies', 'pork', 'dry', 'rice']\n"
     ]
    }
   ],
   "source": [
    "# print('------------------New topic finding!------------------')\n",
    "\n",
    "topic_cand = defaultdict(int)\n",
    "for topic in parent_result:\n",
    "    for ent in ent_ent_index[topic]:\n",
    "        topic_cand[ent] += 1\n",
    "# topic_cand = sorted(topic_cand.items(), key = lambda x: x[1], reverse=True)\n",
    "topic_cand = [x for x in topic_cand if topic_cand[x] >= len(parent_result)/2]\n",
    "\n",
    "remove_list = []\n",
    "for topic in child_entities_count:\n",
    "    remove_list.extend(child_entities_count[topic])\n",
    "# print(remove_list)\n",
    "remove_list.extend(parent_result)\n",
    "\n",
    "tmp = []\n",
    "for topic in topic_cand:\n",
    "    if topic not in remove_list:\n",
    "        tmp.append(topic)\n",
    "topic_cand = tmp\n",
    "\n",
    "topic_entity_ratio_alltopics = {}\n",
    "topic_entity_count_alltopics = {}\n",
    "\n",
    "\n",
    "for test_topic in parent_result:\n",
    "    print(f'test topic: {test_topic}')\n",
    "    \n",
    "    test_data = process_test_data([test_topic], list(topic_cand), max_seq_length)\n",
    "    print(f\"test data point number: {len(test_data)}\")\n",
    "    if len(test_data) > 10000:\n",
    "        test_data = sample(test_data, 10000)   \n",
    "    \n",
    "    entity_ratio, entity_count = relation_inference(test_data, TEST_BATCH_SIZE,mode='child')\n",
    "    topic_entity_ratio_alltopics[test_topic] = entity_ratio\n",
    "    topic_entity_count_alltopics[test_topic] = entity_count\n",
    "\n",
    "topic_entities_count = sum_all_rel(parent_result, topic_entity_count_alltopics, mode='child')\n",
    "\n",
    "# print(topic_entities_count)\n",
    "\n",
    "\n",
    "topic_entities = get_threshold_from_dict(topic_entities_count, 1/3)\n",
    "cap_list = [word_cap[x] for x in topic_hier['ROOT']]\n",
    "print([(x, word_cap[x]) for x in topic_entities if x in word_cap])\n",
    "topic_entities = [x for x in topic_entities if word_cap[x] < max(cap_list)*1.0 and word_cap[x] > min(cap_list)*1.0]\n",
    "for t in topic_hier['ROOT']:\n",
    "    if t in topic_hier:\n",
    "        for t1 in topic_hier[t]:\n",
    "            if t1 in topic_entities:\n",
    "                topic_entities.remove(t1)\n",
    "print(topic_entities)\n",
    "\n",
    "# topic_entities = [x for x in topic_entities if word_cap[x] < max(cap_list) and word_cap[x] > min(cap_list)]\n",
    "# topic_entities = type_consistent_for_list(topic_entities, rep_words, ename2embed_bert, False)\n",
    "# print(topic_entities)\n",
    "for t in topic_hier['ROOT']:\n",
    "    if t in topic_hier:\n",
    "        for t1 in topic_hier[t]:\n",
    "            if t1 in topic_entities:\n",
    "                topic_entities.remove(t1)\n",
    "    for t1 in child_entities[t]:\n",
    "        if t1 in topic_entities:\n",
    "            topic_entities.remove(t1)\n",
    "print(topic_entities)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting positive samples!\n",
      "test data point number: 6406\n",
      "collecting positive samples!\n",
      "test data point number: 21444\n",
      "collecting positive samples!\n",
      "collecting positive samples!\n",
      "test data point number: 13898\n",
      "collecting positive samples!\n",
      "test data point number: 21344\n",
      "collecting positive samples!\n",
      "test data point number: 23296\n",
      "collecting positive samples!\n",
      "test data point number: 29354\n",
      "collecting positive samples!\n",
      "test data point number: 24366\n",
      "collecting positive samples!\n",
      "test data point number: 5866\n",
      "collecting positive samples!\n",
      "test data point number: 12046\n",
      "collecting positive samples!\n",
      "test data point number: 23326\n",
      "collecting positive samples!\n",
      "test data point number: 8708\n",
      "collecting positive samples!\n",
      "test data point number: 15468\n",
      "collecting positive samples!\n",
      "test data point number: 17924\n",
      "collecting positive samples!\n",
      "test data point number: 19322\n",
      "collecting positive samples!\n",
      "test data point number: 8006\n",
      "collecting positive samples!\n",
      "test data point number: 5668\n",
      "collecting positive samples!\n",
      "test data point number: 23602\n",
      "collecting positive samples!\n",
      "test data point number: 5740\n",
      "collecting positive samples!\n",
      "test data point number: 13186\n",
      "collecting positive samples!\n",
      "test data point number: 9044\n",
      "collecting positive samples!\n",
      "test data point number: 20230\n"
     ]
    }
   ],
   "source": [
    "topic_hier1 = {}\n",
    "\n",
    "topic_hier1['ROOT']= topic_entities\n",
    "for topic in topic_hier:\n",
    "    if topic == 'ROOT':\n",
    "        for t in topic_hier[topic]:\n",
    "            if t not in topic_hier1[topic]:\n",
    "                topic_hier1[topic].append(t)\n",
    "    else:\n",
    "        topic_hier1[topic] = [x for x in topic_hier[topic]]\n",
    "        \n",
    "entity_ratio_alltopics1 = {}\n",
    "entity_count_alltopics1 = {}\n",
    "\n",
    "for test_topic in topic_hier1['ROOT']:\n",
    "    if test_topic in topic_hier['ROOT']:\n",
    "        entity_ratio_alltopics1[test_topic] = entity_ratio_alltopics[test_topic]\n",
    "        entity_count_alltopics1[test_topic] = entity_count_alltopics[test_topic]\n",
    "        continue\n",
    "    \n",
    "    sim_ranking = topic_sim(test_topic, vocabulary_inv, topic_emb, word_emb)\n",
    "    cap_ranking, target_cap = rank_cap_customed(word_cap, vocabulary_inv, [vocabulary[word] for word in topic_hier[train_topic]])\n",
    "    coefficient = max(word_cap[test_topic] / word_cap[train_topic],1)\n",
    "    test_cand = aggregate_ranking(sim_ranking, cap_ranking, word_cap, test_topic, vocabulary_inv, pretrain, target_cap*coefficient)\n",
    "\n",
    "    test_data = process_test_data([test_topic], test_cand, max_seq_length)\n",
    "    print(f\"test data point number: {len(test_data)}\")\n",
    "    \n",
    "#     if len(test_data) > 10000:\n",
    "#         test_data=test_data[:10000]\n",
    "\n",
    "    entity_ratio, entity_count = relation_inference(test_data, TEST_BATCH_SIZE)\n",
    "    entity_ratio_alltopics1[test_topic] = entity_ratio\n",
    "    entity_count_alltopics1[test_topic] = entity_count\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROOT': ['red', 'fries', 'cheese', 'bacon', 'burger', 'spicy', 'steak', 'beef', 'sandwich', 'cut', 'fish', 'pizza', 'extra', 'coffee', 'bread', 'hot', 'bowl', 'white', 'shrimp', 'veggies', 'pork', 'dry', 'rice', 'seafood', 'salad', 'dessert'], 'dessert': ['ice_cream', 'cake', 'pastries']}\n"
     ]
    }
   ],
   "source": [
    "# subtopic finding for new topics!\n",
    "topic_hier1 = {}\n",
    "\n",
    "topic_hier1['ROOT']= topic_entities\n",
    "for topic in topic_hier:\n",
    "    if topic == 'ROOT':\n",
    "        for t in topic_hier[topic]:\n",
    "            if t not in topic_hier1[topic]:\n",
    "                topic_hier1[topic].append(t)\n",
    "    else:\n",
    "        topic_hier1[topic] = [x for x in topic_hier[topic]]\n",
    "# print(topic_hier)\n",
    "print(topic_hier1)\n",
    "save_tree_to_file(topic_hier1, 'intermediate.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test topic: red\n",
      "collecting positive samples!\n",
      "test data point number: 6406\n",
      "test topic: fries\n",
      "collecting positive samples!\n",
      "test data point number: 21444\n",
      "test topic: cheese\n",
      "collecting positive samples!\n",
      "test data point number: 45000\n",
      "test topic: bacon\n",
      "collecting positive samples!\n",
      "test data point number: 13898\n",
      "test topic: spicy\n",
      "collecting positive samples!\n",
      "test data point number: 21344\n",
      "test topic: steak\n",
      "collecting positive samples!\n",
      "test data point number: 23296\n",
      "test topic: beef\n",
      "collecting positive samples!\n",
      "test data point number: 29354\n",
      "test topic: sandwich\n",
      "collecting positive samples!\n",
      "test data point number: 24366\n",
      "test topic: cut\n",
      "collecting positive samples!\n",
      "test data point number: 5866\n",
      "test topic: fish\n",
      "collecting positive samples!\n",
      "test data point number: 12046\n",
      "test topic: pizza\n",
      "collecting positive samples!\n",
      "test data point number: 23326\n",
      "test topic: extra\n",
      "collecting positive samples!\n",
      "test data point number: 8708\n",
      "test topic: coffee\n",
      "collecting positive samples!\n",
      "test data point number: 15468\n",
      "test topic: bread\n",
      "collecting positive samples!\n",
      "test data point number: 17924\n",
      "test topic: hot\n",
      "collecting positive samples!\n",
      "test data point number: 19322\n",
      "test topic: bowl\n",
      "collecting positive samples!\n",
      "test data point number: 8006\n",
      "test topic: white\n",
      "collecting positive samples!\n",
      "test data point number: 5668\n",
      "test topic: shrimp\n",
      "collecting positive samples!\n",
      "test data point number: 23602\n",
      "test topic: veggies\n",
      "collecting positive samples!\n",
      "test data point number: 5740\n",
      "test topic: pork\n",
      "collecting positive samples!\n",
      "test data point number: 13186\n",
      "test topic: dry\n",
      "collecting positive samples!\n",
      "test data point number: 9044\n",
      "test topic: rice\n",
      "collecting positive samples!\n",
      "test data point number: 20230\n",
      "-----\n",
      "377\n",
      "-----\n",
      "0\n",
      "[('0', 'martini'), ('3', 'ale'), ('3', 'ale'), ('79', 'cocktail'), ('190', 'smoothie'), ('0', 'martini'), ('356', 'latte'), ('359', 'soda'), ('0', 'martini'), ('367', 'milkshake')]\n",
      "1\n",
      "[('1', 'carrots'), ('1', 'carrots'), ('246', 'snow_peas')]\n",
      "2\n",
      "[('1', 'jalapeno'), ('11', 'tomatillo'), ('11', 'chilli'), ('1', 'jalapeno'), ('31', 'chive'), ('32', 'pimento'), ('35', 'fig'), ('41', 'shoestring'), ('45', 'gouda'), ('45', 'smoked_gouda'), ('45', 'dijon'), ('47', 'peppercorn'), ('1', 'jalapeno'), ('48', 'hatch'), ('1', 'jalapeno'), ('66', 'red_chili'), ('11', 'tomatillo'), ('69', 'pesto'), ('70', 'herb'), ('72', 'scallion'), ('75', 'hot_pepper'), ('47', 'peppercorn'), ('69', 'pesto'), ('47', 'peppercorn'), ('69', 'pesto'), ('116', 'cranberry'), ('45', 'dijon'), ('45', 'gouda'), ('70', 'herb'), ('145', 'clam'), ('69', 'pesto'), ('162', 'tomato_basil'), ('182', 'chili'), ('70', 'herb'), ('35', 'fig'), ('209', 'mushroom'), ('145', 'clam'), ('69', 'pesto'), ('1', 'jalapeno'), ('69', 'pesto'), ('70', 'herb'), ('145', 'clam'), ('66', 'red_chili'), ('72', 'scallion'), ('288', 'furikake'), ('145', 'clam'), ('315', 'beet'), ('315', 'butternut_squash'), ('182', 'chili'), ('70', 'herb'), ('209', 'mushroom'), ('35', 'fig')]\n",
      "3\n",
      "[('2', 'pearl'), ('9', 'copper'), ('72', 'dragon'), ('156', 'federal'), ('171', 'rock'), ('172', 'roman'), ('211', 'spring'), ('72', 'dragon'), ('226', 'saigon'), ('230', 'garden'), ('236', 'gold'), ('292', 'snow'), ('299', 'ocean'), ('354', 'baby'), ('360', 'rose')]\n",
      "4\n",
      "[('2', 'hints'), ('2', 'hints')]\n",
      "5\n",
      "[('3', 'blonde'), ('239', 'blond'), ('3', 'blonde')]\n",
      "6\n",
      "[('5', 'oregano'), ('45', 'earl'), ('5', 'oregano'), ('187', 'joe')]\n",
      "7\n",
      "[('1', 'red_onion'), ('10', 'cheddar_cheese'), ('20', 'american_cheese'), ('22', 'gruyere_cheese'), ('24', 'jack_cheese'), ('10', 'cheddar_cheese'), ('32', 'smoked_bacon'), ('10', 'cheddar_cheese'), ('20', 'american_cheese'), ('45', 'swiss_cheese'), ('32', 'smoked_bacon'), ('22', 'gruyere_cheese'), ('110', 'cranberry_sauce'), ('45', 'swiss_cheese'), ('20', 'american_cheese'), ('10', 'cheddar_cheese'), ('45', 'swiss_cheese'), ('1', 'red_onion'), ('10', 'cheddar_cheese'), ('1', 'red_onion')]\n",
      "8\n",
      "[('11', 'sprinkled'), ('242', 'drizzled'), ('11', 'sprinkled')]\n",
      "9\n",
      "[('13', 'priest'), ('13', 'priest')]\n",
      "10\n",
      "[('13', 'hot_wings'), ('13', 'hot_wings')]\n",
      "11\n",
      "[('5', 'roasted_garlic'), ('19', 'parmesan'), ('19', 'rosemary'), ('19', 'thyme'), ('36', 'balsamic'), ('19', 'parmesan'), ('69', 'basil'), ('5', 'roasted_garlic'), ('19', 'parmesan'), ('69', 'basil'), ('36', 'balsamic'), ('19', 'parmesan'), ('36', 'balsamic'), ('141', 'cornmeal'), ('150', 'panko'), ('159', 'parm'), ('69', 'basil'), ('5', 'roasted_garlic'), ('160', 'fennel'), ('160', 'anchovy'), ('19', 'parmesan'), ('19', 'parmesan'), ('36', 'balsamic'), ('19', 'rosemary'), ('5', 'roasted_garlic'), ('69', 'basil'), ('69', 'basil'), ('160', 'fennel'), ('5', 'roasted_garlic'), ('19', 'parmesan'), ('159', 'parm'), ('19', 'rosemary'), ('69', 'basil'), ('19', 'parmesan'), ('36', 'balsamic'), ('317', 'basil_pesto'), ('160', 'fennel'), ('5', 'roasted_garlic'), ('160', 'anchovy'), ('19', 'parmesan')]\n",
      "12\n",
      "[('19', 'breast')]\n",
      "13\n",
      "[('1', 'fresh_basil'), ('11', 'chili_powder'), ('19', 'parmesan_cheese'), ('19', 'parmesan_cheese'), ('19', 'parmesan_cheese'), ('1', 'fresh_basil'), ('160', 'olive_oil'), ('19', 'parmesan_cheese'), ('160', 'balsamic_vinegar'), ('19', 'parmesan_cheese'), ('160', 'olive_oil'), ('160', 'balsamic_vinegar'), ('19', 'parmesan_cheese'), ('223', 'sesame_seeds'), ('1', 'fresh_basil'), ('160', 'olive_oil'), ('223', 'sesame_seeds'), ('160', 'olive_oil'), ('1', 'fresh_basil'), ('223', 'sesame_seeds'), ('160', 'olive_oil'), ('160', 'balsamic_vinegar'), ('19', 'parmesan_cheese')]\n",
      "14\n",
      "[('12', 'tomato_paste'), ('20', 'vinegar'), ('20', 'vinegar'), ('20', 'vinegar'), ('182', 'pepper'), ('20', 'vinegar'), ('20', 'vinegar'), ('20', 'vinegar'), ('182', 'pepper'), ('20', 'vinegar'), ('182', 'pepper'), ('20', 'vinegar'), ('182', 'pepper')]\n",
      "15\n",
      "[('22', 'caramelized_onions'), ('22', 'carmelized_onions'), ('30', 'grilled_onions'), ('22', 'caramelized_onions'), ('32', 'applewood_smoked_bacon'), ('32', 'pepper_jack_cheese'), ('32', 'saut_ed_mushrooms'), ('36', 'roasted_red_peppers'), ('22', 'caramelized_onions'), ('22', 'carmelized_onions'), ('32', 'applewood_smoked_bacon'), ('45', 'sauteed_onions'), ('47', 'saut_ed_onions'), ('30', 'grilled_onions'), ('32', 'pepper_jack_cheese'), ('32', 'saut_ed_mushrooms'), ('22', 'caramelized_onions'), ('30', 'grilled_onions'), ('22', 'caramelized_onions'), ('22', 'caramelized_onions'), ('30', 'grilled_onions'), ('22', 'carmelized_onions'), ('36', 'roasted_red_peppers'), ('160', 'roasted_peppers'), ('22', 'caramelized_onions'), ('22', 'caramelized_onions'), ('36', 'roasted_red_peppers'), ('45', 'sauteed_onions'), ('36', 'roasted_red_peppers'), ('160', 'roasted_peppers'), ('315', 'candied_pecans'), ('36', 'roasted_red_peppers')]\n",
      "16\n",
      "[('13', 'popper'), ('13', 'popper'), ('13', 'popper')]\n",
      "17\n",
      "[('16', 'gyro_meat'), ('30', 'slaw'), ('30', 'slaw'), ('30', 'slaw'), ('80', 'potato_puree'), ('16', 'gyro_meat'), ('30', 'slaw'), ('109', 'toasted_bread'), ('30', 'slaw'), ('109', 'toasted_bread'), ('30', 'slaw'), ('150', 'remoulade'), ('16', 'gyro_meat'), ('109', 'toasted_bread'), ('217', 'egg_roll'), ('217', 'eggroll'), ('30', 'slaw'), ('30', 'slaw'), ('16', 'gyro_meat'), ('311', 'vinaigrette_dressing')]\n",
      "18\n",
      "[('20', 'strings'), ('20', 'strings'), ('20', 'strings')]\n",
      "19\n",
      "[('11', 'shredded_cheese'), ('17', 'melted_cheese'), ('22', 'gooey_cheese'), ('17', 'melted_cheese'), ('17', 'melted_cheese'), ('17', 'melted_cheese'), ('17', 'melted_cheese'), ('17', 'melted_cheese'), ('17', 'melted_cheese')]\n",
      "20\n",
      "[('33', 'compote'), ('33', 'compote'), ('33', 'compote'), ('33', 'compote')]\n",
      "21\n",
      "[('33', 'whipped_cream'), ('45', 'maple_syrup'), ('185', 'condensed_milk'), ('33', 'whipped_cream'), ('45', 'maple_syrup'), ('199', 'strawberry_jam'), ('344', 'chocolate_chips'), ('33', 'whipped_cream'), ('346', 'fresh_berries'), ('45', 'maple_syrup'), ('348', 'hot_fudge'), ('348', 'chocolate_syrup'), ('367', 'whip_cream'), ('367', 'powdered_sugar')]\n",
      "22\n",
      "[('12', 'saucy'), ('34', 'runny'), ('34', 'runny'), ('67', 'watery'), ('93', 'chewy'), ('34', 'runny'), ('150', 'fishy'), ('168', 'doughy'), ('12', 'saucy'), ('168', 'floppy'), ('168', 'bready'), ('186', 'bitter'), ('34', 'runny'), ('67', 'watery'), ('93', 'chewy'), ('186', 'bitter'), ('34', 'runny'), ('186', 'bitter')]\n",
      "23\n",
      "[('35', 'confit'), ('35', 'confit')]\n",
      "24\n",
      "[('2', 'walnut'), ('37', 'pecan'), ('127', 'raisin'), ('37', 'pecan'), ('2', 'walnut'), ('37', 'pecan'), ('2', 'walnut'), ('37', 'pecan'), ('344', 'praline'), ('344', 'nut'), ('127', 'raisin')]\n",
      "25\n",
      "[('38', 'rubbery'), ('93', 'tough'), ('93', 'burnt'), ('93', 'overdone'), ('38', 'rubbery'), ('93', 'slightly_overcooked'), ('93', 'underdone'), ('93', 'overdone'), ('38', 'rubbery'), ('153', 'gummy'), ('93', 'burnt'), ('237', 'sticky'), ('38', 'rubbery'), ('38', 'rubbery'), ('273', 'flavourless'), ('153', 'gummy'), ('237', 'sticky'), ('38', 'rubbery'), ('312', 'soggy'), ('312', 'soggy')]\n",
      "26\n",
      "[('9', 'label'), ('39', 'grind'), ('66', 'burn'), ('85', 'comp'), ('128', 'stock'), ('85', 'comp'), ('187', 'brew'), ('218', 'slurp'), ('293', 'peel'), ('85', 'comp')]\n",
      "27\n",
      "[('13', 'beef_patties'), ('20', 'spuds'), ('39', 'patties'), ('13', 'beef_patties'), ('40', 'buns'), ('41', 'fires'), ('41', 'burger_patties'), ('40', 'buns'), ('39', 'patties'), ('39', 'patties'), ('40', 'buns'), ('142', 'taco_shells'), ('142', 'tortillas'), ('40', 'buns'), ('156', 'crusts'), ('159', 'knots'), ('159', 'knots'), ('210', 'meats'), ('142', 'tortillas'), ('40', 'buns'), ('142', 'tortillas'), ('40', 'buns'), ('300', 'shells'), ('142', 'tortillas')]\n",
      "28\n",
      "[('13', 'nuggets'), ('13', 'nuggets'), ('13', 'nuggets')]\n",
      "29\n",
      "[('42', 'pulled_pork'), ('42', 'brisket'), ('42', 'tri_tip'), ('42', 'pulled_chicken'), ('55', 'pastrami'), ('55', 'corned_beef'), ('59', 'hash_brown'), ('42', 'brisket'), ('42', 'brisket'), ('42', 'pulled_pork'), ('42', 'pulled_chicken'), ('102', 'bbq_chicken'), ('55', 'pastrami'), ('110', 'turkey'), ('110', 'roast_beef'), ('55', 'corned_beef'), ('110', 'smoked_turkey'), ('116', 'peameal_bacon'), ('42', 'brisket'), ('42', 'pulled_chicken'), ('55', 'corned_beef'), ('110', 'turkey'), ('192', 'lox'), ('192', 'lox'), ('42', 'pulled_chicken'), ('42', 'brisket'), ('299', 'smoked_salmon')]\n",
      "30\n",
      "[('45', 'rocket')]\n",
      "31\n",
      "[('45', 'piled_high'), ('45', 'piled_high'), ('231', 'garnished')]\n",
      "32\n",
      "[('47', 'wheat_bun'), ('47', 'toasted_bun')]\n",
      "33\n",
      "[('47', 'char'), ('47', 'char'), ('90', 'sear'), ('47', 'char'), ('109', 'crunch'), ('47', 'char'), ('149', 'light_batter'), ('47', 'char'), ('149', 'crispy_skin'), ('159', 'crispy_crust'), ('149', 'crispy_skin'), ('47', 'char'), ('90', 'sear')]\n",
      "34\n",
      "[('47', 'dripping'), ('47', 'dripping')]\n",
      "35\n",
      "[('47', 'cooked_correctly'), ('47', 'cooked_correctly'), ('93', 'cooked_properly'), ('47', 'cooked_correctly'), ('93', 'cooked_properly')]\n",
      "36\n",
      "[('47', 'fixings'), ('48', 'condiments'), ('48', 'condiments'), ('116', 'juices'), ('47', 'fixings'), ('204', 'dipping_sauces'), ('216', 'add_ins'), ('359', 'beverages')]\n",
      "37\n",
      "[('49', 'ribeye'), ('49', 'sirloin'), ('49', 'filet_mignon'), ('49', 'skirt_steak'), ('49', 'sirloin'), ('77', 'tenderloin'), ('77', 'pork_tenderloin'), ('78', 'porterhouse'), ('78', 'bone_in_rib_eye'), ('78', 'lobster_tail'), ('78', 'fillet_mignon'), ('80', 'grilled_salmon'), ('49', 'skirt_steak'), ('116', 'croque_monsieur'), ('117', 'roasted_chicken'), ('130', 'rib_eye'), ('130', 'halibut'), ('49', 'ribeye'), ('141', 'seabass'), ('157', 'gnocchi'), ('157', 'linguini'), ('162', 'ziti'), ('162', 'eggplant_parm'), ('162', 'chicken_parmigiana'), ('192', 'croque_madame'), ('245', 'sea_scallops'), ('263', 'beef_rib'), ('49', 'skirt_steak'), ('130', 'rib_eye'), ('77', 'pork_tenderloin'), ('271', 'chile_relleno'), ('272', 'pork_shank'), ('49', 'sirloin'), ('157', 'gnocchi'), ('295', 'meatloaf'), ('78', 'lobster_tail'), ('130', 'rib_eye'), ('49', 'ribeye'), ('130', 'halibut'), ('301', 'trout'), ('302', 'sea_bass'), ('141', 'seabass'), ('245', 'sea_scallops'), ('303', 'grilled_octopus'), ('308', 'dragon_roll'), ('157', 'gnocchi')]\n",
      "38\n",
      "[('18', 'yucca'), ('48', 'cajun'), ('52', 'truffle'), ('52', 'truffle'), ('52', 'truffle'), ('48', 'cajun')]\n",
      "39\n",
      "[('53', 'burger_joints'), ('78', 'steakhouses'), ('109', 'sandwich_shops'), ('156', 'chains'), ('173', 'shops')]\n",
      "40\n",
      "[('41', 'naked'), ('54', 'pink'), ('54', 'pink'), ('83', 'blue'), ('54', 'pink'), ('203', 'plastic'), ('208', 'wet'), ('208', 'golden'), ('233', 'yellow'), ('54', 'pink'), ('237', 'purple'), ('237', 'coloured'), ('279', 'brown'), ('54', 'pink'), ('279', 'grey'), ('279', 'brown'), ('367', 'flat')]\n",
      "41\n",
      "[('6', 'marinara'), ('20', 'mayonnaise'), ('20', 'kraut'), ('20', 'dijon_mustard'), ('20', 'kraut'), ('22', 'tomato_jam'), ('24', 'processed_cheese'), ('25', 'cream_cheese'), ('31', 'creme_fraiche'), ('32', 'mustard'), ('45', 'crispy_bacon'), ('22', 'tomato_jam'), ('47', 'thousand_island_dressing'), ('20', 'mayonnaise'), ('20', 'dijon_mustard'), ('48', 'sriracha'), ('32', 'mustard'), ('20', 'kraut'), ('55', 'sauerkraut'), ('48', 'sriracha'), ('70', 'mayo'), ('32', 'mustard'), ('20', 'mayonnaise'), ('100', 'oyster_sauce'), ('70', 'mayo'), ('32', 'mustard'), ('55', 'sauerkraut'), ('70', 'mayo'), ('32', 'mustard'), ('55', 'sauerkraut'), ('47', 'thousand_island_dressing'), ('20', 'mayonnaise'), ('20', 'kraut'), ('20', 'dijon_mustard'), ('116', 'spicy_mustard'), ('25', 'cream_cheese'), ('70', 'mayo'), ('20', 'mayonnaise'), ('151', 'spicy_mayo'), ('151', 'nori'), ('6', 'marinara'), ('70', 'mayo'), ('151', 'nori'), ('32', 'mustard'), ('70', 'mayo'), ('116', 'spicy_mustard'), ('151', 'spicy_mayo'), ('48', 'sriracha'), ('151', 'nori'), ('6', 'marinara'), ('25', 'cream_cheese'), ('151', 'spicy_mayo'), ('151', 'spicy_mayo'), ('20', 'mayonnaise'), ('55', 'sauerkraut'), ('32', 'mustard'), ('151', 'spicy_mayo'), ('151', 'nori'), ('311', 'balsamic_vinaigrette'), ('311', 'caesar_dressing')]\n",
      "42\n",
      "[('40', 'sonoran'), ('45', 'cowboy'), ('57', 'hawaiian'), ('62', 'hakka'), ('154', 'deep_dish'), ('156', 'neapolitan'), ('162', 'authentic_italian'), ('168', 'ny_style'), ('182', 'thai'), ('182', 'complementary'), ('194', 'typical_diner'), ('265', 'szechuan'), ('283', 'shanghai'), ('298', 'fusion'), ('298', 'malaysian'), ('342', 'french'), ('357', 'italian'), ('361', 'japanese'), ('370', 'mexican')]\n",
      "43\n",
      "[('34', 'sausage_patties'), ('59', 'home_fries'), ('59', 'hash_browns'), ('59', 'scrambled_eggs'), ('59', 'hashbrowns'), ('59', 'hash_browns'), ('59', 'home_fries'), ('59', 'scrambled_eggs'), ('34', 'sausage_patties'), ('59', 'hash_browns'), ('59', 'home_fries'), ('59', 'scrambled_eggs'), ('59', 'hashbrowns')]\n",
      "44\n",
      "[('39', 'beef_patty'), ('41', 'pickle'), ('45', 'fried_egg'), ('49', 'baked_potato'), ('59', 'biscuit'), ('74', 'sizzling_plate'), ('112', 'waffle'), ('119', 'hot_link'), ('134', 'baguette'), ('59', 'biscuit'), ('188', 'veggie_omelet'), ('204', 'soft_pretzel'), ('217', 'spring_roll'), ('231', 'boiled_egg'), ('231', 'poached_egg'), ('231', 'soft_boiled_egg'), ('231', 'boiled_egg'), ('45', 'fried_egg'), ('231', 'boiled_egg'), ('278', 'chicken_thigh'), ('278', 'chicken_leg'), ('112', 'waffle')]\n",
      "45\n",
      "[('43', 'club_sandwich'), ('43', 'reuben_sandwich'), ('59', 'breakfast_burrito'), ('110', 'bobbie'), ('111', 'stromboli'), ('111', 'stromboli'), ('186', 'chai_latte'), ('111', 'stromboli'), ('344', 'red_velvet_cupcake'), ('356', 'cappuccino'), ('367', 'brownie_sundae')]\n",
      "46\n",
      "[('60', 'draft')]\n",
      "47\n",
      "[('62', 'spices'), ('67', 'seasonings'), ('62', 'spices'), ('94', 'herbs'), ('67', 'seasonings'), ('67', 'seasonings'), ('62', 'spices'), ('94', 'herbs'), ('62', 'spices'), ('67', 'seasonings'), ('256', 'oils'), ('67', 'seasonings'), ('94', 'herbs'), ('67', 'seasonings'), ('62', 'spices'), ('94', 'herbs')]\n",
      "48\n",
      "[('2', 'emerald'), ('62', 'jerk'), ('75', 'buffalo'), ('117', 'rotisserie'), ('226', 'cashew'), ('226', 'cashew'), ('62', 'jerk'), ('293', 'maine'), ('303', 'firecracker')]\n",
      "49\n",
      "[('4', 'prawns'), ('13', 'poppers'), ('47', 'wedges'), ('62', 'pakoras'), ('63', 'dumplings'), ('63', 'wontons'), ('78', 'sliders'), ('86', 'enchiladas'), ('47', 'wedges'), ('86', 'enchiladas'), ('151', 'skewers'), ('160', 'hearts'), ('63', 'wontons'), ('244', 'eggrolls'), ('265', 'fried_noodles'), ('4', 'prawns'), ('151', 'skewers'), ('86', 'enchiladas'), ('160', 'hearts'), ('349', 'pancakes'), ('376', 'tacos')]\n",
      "50\n",
      "[('7', 'sticky_rice'), ('7', 'sticky_rice'), ('7', 'sticky_rice'), ('7', 'sticky_rice'), ('7', 'sticky_rice'), ('287', 'lassi'), ('7', 'sticky_rice'), ('7', 'sticky_rice')]\n",
      "51\n",
      "[('7', 'stew'), ('64', 'noodle_soup'), ('64', 'noodle_soup'), ('157', 'carpaccio'), ('64', 'noodle_soup'), ('64', 'noodle_soup'), ('7', 'stew'), ('64', 'noodle_soup'), ('157', 'carpaccio'), ('287', 'pakora'), ('157', 'carpaccio')]\n",
      "52\n",
      "[('34', 'deliciousness'), ('42', 'barbecue'), ('50', 'frozen_custard'), ('53', 'jr'), ('64', 'hot_pot'), ('64', 'pho'), ('90', 'red_meat'), ('115', 'shwarma'), ('141', 'perch'), ('143', 'sake'), ('183', 'product'), ('190', 'boba_tea'), ('194', 'hookah'), ('219', 'bun_bo_hue'), ('220', 'congee'), ('225', 'boba'), ('293', 'stone_crab'), ('298', 'asian_cuisine'), ('340', 'sharing'), ('348', 'frozen_yogurt')]\n",
      "53\n",
      "[('2', 'coconut_milk'), ('11', 'lime_juice'), ('11', 'chili_flakes'), ('65', 'soy_sauce'), ('65', 'chili_paste'), ('65', 'sesame_oil'), ('11', 'chili_flakes'), ('65', 'hoisin_sauce'), ('11', 'lime_juice'), ('2', 'coconut_milk'), ('75', 'tabasco'), ('65', 'hoisin_sauce'), ('65', 'soy_sauce'), ('102', 'ground_meat'), ('151', 'ponzu_sauce'), ('178', 'soy_milk'), ('181', 'oil'), ('182', 'salt'), ('182', 'sriracha_sauce'), ('178', 'soy_milk'), ('2', 'coconut_milk'), ('181', 'oil'), ('182', 'salt'), ('216', 'crushed_ice'), ('220', 'instant_noodles'), ('65', 'soy_sauce'), ('65', 'chili_paste'), ('2', 'coconut_milk'), ('65', 'hoisin_sauce'), ('65', 'hoisin_sauce'), ('65', 'soy_sauce'), ('65', 'soy_sauce'), ('65', 'soy_sauce'), ('288', 'soya_sauce'), ('65', 'chili_paste'), ('299', 'imitation_crab'), ('181', 'oil')]\n",
      "54\n",
      "[('65', 'sesame'), ('78', 'angus'), ('65', 'sesame'), ('65', 'sesame'), ('65', 'sesame'), ('65', 'sesame')]\n",
      "55\n",
      "[('1', 'green_onions'), ('1', 'scallions'), ('31', 'chives'), ('1', 'green_onions'), ('1', 'scallions'), ('1', 'green_onions'), ('1', 'scallions'), ('1', 'green_onions'), ('1', 'green_onions'), ('1', 'scallions'), ('1', 'green_onions'), ('238', 'pine_nuts'), ('1', 'scallions'), ('1', 'scallions'), ('31', 'chives'), ('1', 'scallions'), ('1', 'green_onions'), ('1', 'green_onions'), ('238', 'pine_nuts'), ('315', 'walnuts')]\n",
      "56\n",
      "[('72', 'minced_pork'), ('72', 'minced_pork'), ('72', 'minced_pork'), ('72', 'minced_pork')]\n",
      "57\n",
      "[('7', 'brown_rice'), ('7', 'jasmine_rice'), ('74', 'white_rice'), ('74', 'mixed_vegetables'), ('80', 'sauteed_spinach'), ('74', 'mixed_vegetables'), ('7', 'brown_rice'), ('154', 'extra_cheese'), ('74', 'white_rice'), ('74', 'white_rice'), ('215', 'steamed_rice'), ('7', 'jasmine_rice'), ('215', 'plain_rice'), ('216', 'fresh_fruit'), ('7', 'brown_rice'), ('74', 'mixed_vegetables'), ('256', 'cottage_cheese'), ('215', 'plain_rice'), ('260', 'egg_whites'), ('7', 'jasmine_rice'), ('215', 'steamed_rice'), ('216', 'fresh_fruit')]\n",
      "58\n",
      "[('8', 'mole'), ('45', 'smash'), ('66', 'chilis'), ('75', 'ranch'), ('75', 'wing_sauce'), ('75', 'ranch'), ('115', 'protein'), ('141', 'bass'), ('142', 'chip'), ('143', 'poki'), ('150', 'battered_fish'), ('159', 'pep'), ('75', 'ranch'), ('186', 'crema'), ('192', 'rye'), ('199', 'jam'), ('216', 'acai'), ('242', 'sprinkles'), ('318', 'sprouts'), ('75', 'ranch'), ('242', 'sprinkles'), ('348', 'candy')]\n",
      "59\n",
      "[('75', 'sweet_potato'), ('75', 'sweet_potato'), ('75', 'sweet_potato'), ('75', 'sweet_potato')]\n",
      "60\n",
      "[('77', 'medallions'), ('77', 'medallions'), ('77', 'medallions')]\n",
      "61\n",
      "[('19', 'breast')]\n",
      "62\n",
      "[('41', 'animal_style'), ('45', 'hog'), ('78', 'dry_aged'), ('78', 'wagyu'), ('90', 'rib'), ('99', 'galbi'), ('123', 'monster'), ('90', 'rib'), ('130', 'striploin'), ('141', 'sole'), ('144', 'pollo'), ('78', 'wagyu'), ('154', 'meat_lovers'), ('154', 'deluxe'), ('208', 'trifecta'), ('144', 'pollo'), ('265', 'kare'), ('293', 'oyster'), ('78', 'wagyu'), ('302', 'toro'), ('302', 'kobe_beef'), ('99', 'galbi'), ('336', 'creme')]\n",
      "63\n",
      "[('1', 'cashews'), ('5', 'artichokes'), ('20', 'banana_peppers'), ('22', 'sweet_peppers'), ('22', 'sauteed_mushrooms'), ('24', 'diced_tomatoes'), ('22', 'sauteed_mushrooms'), ('80', 'mushrooms'), ('80', 'fingerling_potatoes'), ('80', 'grilled_asparagus'), ('22', 'sauteed_mushrooms'), ('80', 'wild_mushrooms'), ('82', 'onions'), ('82', 'tomatoes'), ('20', 'banana_peppers'), ('126', 'feta_cheese'), ('20', 'banana_peppers'), ('5', 'artichokes'), ('126', 'feta_cheese'), ('82', 'onions'), ('82', 'tomatoes'), ('126', 'feta_cheese'), ('80', 'mushrooms'), ('5', 'artichokes'), ('126', 'feta_cheese'), ('82', 'onions'), ('5', 'artichokes'), ('126', 'feta_cheese'), ('254', 'fresh_greens'), ('254', 'roasted_veggies'), ('256', 'chick_peas'), ('82', 'tomatoes'), ('82', 'onions'), ('315', 'pears'), ('126', 'feta_cheese'), ('20', 'banana_peppers'), ('254', 'fresh_greens'), ('318', 'roasted_vegetables'), ('80', 'mushrooms'), ('5', 'artichokes'), ('1', 'cashews'), ('256', 'chick_peas')]\n",
      "64\n",
      "[('30', 'onion_straws'), ('30', 'onion_straws'), ('52', 'sweet_potatoes'), ('72', 'rice_cakes'), ('74', 'green_beans'), ('80', 'mash_potatoes'), ('80', 'whipped_potatoes'), ('80', 'roasted_potatoes'), ('80', 'smashed_potatoes'), ('80', 'scalloped_potatoes'), ('81', 'mashed_potatoes'), ('74', 'green_beans'), ('91', 'poached_eggs'), ('91', 'grits'), ('97', 'fish_balls'), ('74', 'green_beans'), ('81', 'mashed_potatoes'), ('119', 'baked_beans'), ('81', 'mashed_potatoes'), ('80', 'mash_potatoes'), ('74', 'green_beans'), ('72', 'rice_cakes'), ('97', 'fish_balls'), ('74', 'green_beans'), ('97', 'fish_balls'), ('256', 'grilled_veggies'), ('52', 'sweet_potatoes'), ('272', 'spaetzle'), ('81', 'mashed_potatoes'), ('280', 'collard_greens'), ('52', 'sweet_potatoes'), ('72', 'rice_cakes'), ('80', 'mash_potatoes'), ('91', 'grits'), ('97', 'fish_balls'), ('74', 'green_beans'), ('81', 'mashed_potatoes'), ('52', 'sweet_potatoes'), ('81', 'mashed_potatoes')]\n",
      "65\n",
      "[('32', 'multigrain'), ('81', 'corn'), ('109', 'focaccia'), ('32', 'multigrain'), ('127', 'brioche'), ('127', 'wheat'), ('127', 'brioche'), ('142', 'flour'), ('160', 'artichoke'), ('127', 'wheat'), ('81', 'corn'), ('127', 'brioche'), ('192', 'sourdough'), ('192', 'challah'), ('142', 'flour'), ('127', 'wheat'), ('81', 'corn'), ('160', 'artichoke'), ('142', 'flour'), ('263', 'char_siu'), ('142', 'flour'), ('142', 'flour'), ('160', 'artichoke'), ('299', 'quail'), ('81', 'corn'), ('81', 'corn')]\n",
      "66\n",
      "[('87', 'upscale'), ('187', 'independent'), ('194', 'homey'), ('211', 'outdoor')]\n",
      "67\n",
      "[('67', 'flavoring'), ('88', 'seasoning'), ('90', 'marbling'), ('90', 'marbling'), ('183', 'spice'), ('88', 'seasoning'), ('67', 'flavoring'), ('88', 'seasoning'), ('273', 'moisture')]\n",
      "68\n",
      "[('88', 'stated')]\n",
      "69\n",
      "[('89', 'min'), ('180', 'mins')]\n",
      "70\n",
      "[('90', 'lean'), ('90', 'lean'), ('90', 'lean'), ('90', 'lean')]\n",
      "71\n",
      "[('93', 'acceptable'), ('342', 'impressive')]\n",
      "72\n",
      "[('93', 'lacked_flavor'), ('93', 'lacked_flavor'), ('93', 'lacked_flavor')]\n",
      "73\n",
      "[('2', 'blood'), ('90', 'fat'), ('2', 'blood'), ('90', 'fat'), ('2', 'blood'), ('90', 'fat')]\n",
      "74\n",
      "[('90', 'flank'), ('97', 'tendon'), ('97', 'tripe'), ('97', 'rare_beef'), ('97', 'beef_balls'), ('90', 'flank'), ('97', 'tripe'), ('97', 'tendon'), ('97', 'tripe'), ('97', 'tendon'), ('97', 'beef_balls'), ('97', 'tripe'), ('286', 'tendons')]\n",
      "75\n",
      "[('46', 'foie_gras'), ('63', 'soft_shell_crab'), ('73', 'pork_shoulder'), ('74', 'tilapia'), ('77', 'flank_steak'), ('77', 'mahi_mahi'), ('79', 'king_crab'), ('46', 'foie_gras'), ('77', 'flank_steak'), ('99', 'pork_loin'), ('99', 'octopus'), ('99', 'goulash'), ('116', 'smoked_brisket'), ('77', 'flank_steak'), ('46', 'foie_gras'), ('74', 'tilapia'), ('141', 'flounder'), ('142', 'el_pastor'), ('143', 'red_snapper'), ('63', 'soft_shell_crab'), ('143', 'yellow_tail'), ('143', 'seared_tuna'), ('143', 'spicy_tuna'), ('143', 'salmon_belly'), ('145', 'squid'), ('46', 'foie_gras'), ('160', 'burrata'), ('219', 'chashu'), ('143', 'spicy_tuna'), ('221', 'spicy_salmon'), ('245', 'mahi'), ('77', 'flank_steak'), ('99', 'pork_loin'), ('269', 'marrow'), ('99', 'goulash'), ('79', 'king_crab'), ('292', 'crawfish'), ('292', 'dungeness_crab'), ('293', 'mussel'), ('293', 'alligator'), ('297', 'fried_shrimp'), ('297', 'catfish'), ('297', 'fried_fish'), ('299', 'mackerel'), ('299', 'sea_urchin'), ('301', 'swordfish'), ('77', 'mahi_mahi'), ('301', 'rabbit'), ('302', 'snapper'), ('302', 'cod'), ('302', 'hamachi'), ('302', 'butterfish'), ('143', 'seared_tuna'), ('145', 'squid'), ('99', 'octopus'), ('143', 'spicy_tuna'), ('221', 'spicy_salmon'), ('160', 'burrata'), ('46', 'foie_gras')]\n",
      "76\n",
      "[('62', 'vindaloo'), ('99', 'liver'), ('100', 'satay'), ('115', 'shawarma'), ('219', 'sate'), ('115', 'shawarma'), ('100', 'satay'), ('219', 'sate'), ('99', 'liver'), ('99', 'liver'), ('100', 'satay')]\n",
      "77\n",
      "[('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('257', 'pickled_carrots'), ('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('257', 'pickled_carrots'), ('72', 'bean_sprouts'), ('72', 'bean_sprouts'), ('315', 'raisins')]\n",
      "78\n",
      "[('74', 'brown_sauce'), ('100', 'marinade'), ('135', 'batter'), ('135', 'coating'), ('100', 'marinade'), ('100', 'marinade'), ('274', 'rub'), ('100', 'marinade')]\n",
      "79\n",
      "[('100', 'bamboo'), ('100', 'bamboo'), ('242', 'grape'), ('100', 'bamboo')]\n",
      "80\n",
      "[('1', 'carrots')]\n",
      "81\n",
      "[('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers'), ('1', 'cucumbers')]\n",
      "82\n",
      "[('63', 'papaya'), ('65', 'seaweed'), ('65', 'seaweed'), ('65', 'seaweed'), ('65', 'seaweed'), ('65', 'seaweed'), ('63', 'papaya'), ('230', 'kale'), ('65', 'seaweed')]\n",
      "83\n",
      "[('19', 'breast')]\n",
      "84\n",
      "[('24', 'grilled_vegetables'), ('76', 'beans'), ('76', 'beans'), ('76', 'beans'), ('104', 'black_beans'), ('76', 'beans'), ('104', 'black_beans'), ('231', 'pickled_radish'), ('104', 'black_beans'), ('231', 'pickled_radish'), ('104', 'black_beans'), ('76', 'beans'), ('104', 'black_beans')]\n",
      "85\n",
      "[('66', 'chile'), ('66', 'chile'), ('66', 'chile'), ('66', 'chile'), ('66', 'chile')]\n",
      "86\n",
      "[('58', 'melt'), ('58', 'melt'), ('58', 'melt')]\n",
      "87\n",
      "[('2', 'puree'), ('7', 'chutney'), ('20', 'relish'), ('24', 'crumbles'), ('20', 'relish'), ('20', 'relish'), ('20', 'relish'), ('126', 'vinaigrette'), ('223', 'paste'), ('256', 'seeds'), ('223', 'paste'), ('223', 'paste'), ('126', 'vinaigrette')]\n",
      "88\n",
      "[('19', 'breast')]\n",
      "89\n",
      "[('120', 'iced_coffee'), ('120', 'americano'), ('178', 'iced_tea'), ('120', 'americano'), ('186', 'iced_mocha'), ('186', 'italian_soda'), ('178', 'iced_tea'), ('178', 'iced_tea')]\n",
      "90\n",
      "[('1', 'chilies'), ('5', 'olives'), ('7', 'lentils'), ('11', 'chiles'), ('11', 'limes'), ('11', 'peanuts'), ('20', 'pickles'), ('20', 'pickles'), ('28', 'cheeses'), ('28', 'crackers'), ('28', 'cured_meats'), ('5', 'olives'), ('36', 'croutons'), ('45', 'shrooms'), ('63', 'rice_noodles'), ('1', 'chilies'), ('11', 'chiles'), ('11', 'peanuts'), ('102', 'fresh_vegetables'), ('20', 'pickles'), ('20', 'pickles'), ('110', 'cold_cuts'), ('126', 'greens'), ('36', 'croutons'), ('144', 'refried_beans'), ('145', 'clams'), ('5', 'olives'), ('28', 'cheeses'), ('176', 'avocados'), ('20', 'pickles'), ('187', 'oats'), ('199', 'sausages'), ('200', 'crusty_bread'), ('28', 'cheeses'), ('63', 'rice_noodles'), ('144', 'refried_beans'), ('7', 'lentils'), ('126', 'greens'), ('102', 'fresh_vegetables'), ('5', 'olives'), ('126', 'greens'), ('246', 'string_beans'), ('11', 'chiles'), ('252', 'fruits'), ('246', 'string_beans'), ('126', 'greens'), ('256', 'chickpeas'), ('7', 'lentils'), ('5', 'olives'), ('11', 'peanuts'), ('246', 'string_beans'), ('199', 'sausages'), ('270', 'pickled_veggies'), ('252', 'fruits'), ('7', 'lentils'), ('145', 'clams'), ('199', 'sausages'), ('36', 'croutons'), ('315', 'beets'), ('5', 'olives'), ('126', 'greens'), ('20', 'pickles'), ('7', 'lentils'), ('256', 'chickpeas'), ('252', 'fruits')]\n",
      "91\n",
      "[('127', 'muffin'), ('127', 'scone'), ('127', 'scone'), ('192', 'danish'), ('127', 'scone'), ('340', 'budino'), ('342', 'strudel'), ('362', 'croissant'), ('127', 'muffin'), ('192', 'danish'), ('127', 'scone')]\n",
      "92\n",
      "[('128', 'smaller_pieces')]\n",
      "93\n",
      "[('90', 'gristle'), ('96', 'cartilage'), ('90', 'gristle'), ('90', 'gristle'), ('90', 'gristle'), ('96', 'cartilage')]\n",
      "94\n",
      "[('128', 'ensure')]\n",
      "95\n",
      "[('128', 'required')]\n",
      "96\n",
      "[('128', 'fell')]\n",
      "97\n",
      "[('128', 'generously'), ('135', 'lightly')]\n",
      "98\n",
      "[('128', 'intact')]\n",
      "99\n",
      "[('48', 'fully_loaded'), ('78', 'cooked_medium'), ('130', 'cooked_medium_rare')]\n",
      "100\n",
      "[('19', 'breast'), ('19', 'breast')]\n",
      "101\n",
      "[('48', 'lb'), ('49', 'oz'), ('49', 'oz'), ('49', 'oz'), ('154', 'inch_pizza')]\n",
      "102\n",
      "[('130', 'wood'), ('155', 'brick'), ('130', 'wood'), ('236', 'marble'), ('236', 'tile')]\n",
      "103\n",
      "[('23', 'bits'), ('23', 'bits'), ('131', 'cubes'), ('131', 'cubes'), ('235', 'slivers'), ('23', 'bits'), ('131', 'cubes'), ('23', 'bits')]\n",
      "104\n",
      "[('45', 'stack'), ('131', 'chunk'), ('131', 'cube'), ('218', 'packet'), ('131', 'cube'), ('326', 'wedge'), ('367', 'scoop')]\n",
      "105\n",
      "[('132', 'difficult'), ('132', 'impossible')]\n",
      "106\n",
      "[('132', 'easier'), ('132', 'harder')]\n",
      "107\n",
      "[('133', 'melts')]\n",
      "108\n",
      "[('91', 'ham'), ('91', 'ham'), ('91', 'ham'), ('119', 'macaroni'), ('91', 'ham'), ('91', 'ham'), ('91', 'ham'), ('91', 'ham'), ('119', 'macaroni'), ('91', 'ham'), ('91', 'ham')]\n",
      "109\n",
      "[('11', 'rim'), ('135', 'edges'), ('135', 'edges')]\n",
      "110\n",
      "[('24', 'flavor_profile'), ('24', 'flavor_profile'), ('128', 'doneness'), ('135', 'consistency'), ('153', 'color'), ('195', 'aroma'), ('195', 'aroma'), ('258', 'freshness'), ('153', 'color'), ('258', 'freshness')]\n",
      "111\n",
      "[('12', 'canned'), ('47', 'smashed'), ('72', 'pan_fried'), ('78', 'aged'), ('80', 'seared'), ('93', 'raw'), ('80', 'seared'), ('135', 'deep_fried'), ('141', 'pan_seared'), ('182', 'leftover'), ('208', 'black'), ('208', 'fiery'), ('208', 'cast_iron'), ('273', 'pre_cooked'), ('278', 'boneless'), ('293', 'jumbo'), ('80', 'seared'), ('309', 'steam'), ('350', 'assorted'), ('208', 'black')]\n",
      "112\n",
      "[('13', 'brussel_sprouts'), ('41', 'loaded_fries'), ('13', 'brussel_sprouts'), ('52', 'brussels_sprouts'), ('62', 'lamb_skewers'), ('63', 'potstickers'), ('63', 'lettuce_wraps'), ('68', 'mussels'), ('68', 'scallops'), ('68', 'scallops'), ('80', 'potatoes_au_gratin'), ('13', 'brussel_sprouts'), ('80', 'brussels'), ('99', 'pork_ribs'), ('99', 'beef_short_ribs'), ('119', 'hot_links'), ('137', 'short_ribs'), ('141', 'fried_oysters'), ('141', 'jumbo_shrimp'), ('144', 'chicken_enchiladas'), ('68', 'mussels'), ('172', 'rice_balls'), ('63', 'lettuce_wraps'), ('244', 'fish_cakes'), ('264', 'meat_balls'), ('13', 'brussel_sprouts'), ('272', 'pork_chops'), ('293', 'crab_cakes'), ('141', 'jumbo_shrimp'), ('293', 'frog_legs'), ('141', 'fried_oysters'), ('295', 'stuffed_mushrooms'), ('264', 'meat_balls'), ('296', 'lamb_chops'), ('272', 'pork_chops'), ('63', 'potstickers'), ('309', 'fried_dumplings'), ('52', 'brussels_sprouts')]\n",
      "113\n",
      "[('138', 'mush'), ('138', 'mush'), ('159', 'cheesy_goodness'), ('138', 'mush'), ('138', 'mush')]\n",
      "114\n",
      "[('138', 'rubber'), ('138', 'cardboard'), ('138', 'cardboard')]\n",
      "115\n",
      "[('128', 'edge'), ('138', 'surface'), ('208', 'stove')]\n",
      "116\n",
      "[('138', 'straw'), ('221', 'bug'), ('138', 'straw')]\n",
      "117\n",
      "[('12', 'stone'), ('41', 'reg'), ('139', 'square'), ('139', 'shaped'), ('139', 'circle'), ('173', 'nickel'), ('200', 'port'), ('211', 'cooler'), ('211', 'center'), ('237', 'esque'), ('211', 'center'), ('345', 'grill')]\n",
      "118\n",
      "[('139', 'knives'), ('203', 'forks'), ('139', 'knives'), ('223', 'chopsticks')]\n",
      "119\n",
      "[('139', 'shirt'), ('139', 'shirt'), ('239', 'jeans'), ('239', 'pants')]\n",
      "120\n",
      "[('8', 'tamales'), ('9', 'reds'), ('34', 'red_velvet_pancakes'), ('40', 'hot_dogs'), ('76', 'fajitas'), ('140', 'oysters'), ('140', 'crab_legs'), ('141', 'fishes'), ('76', 'fajitas'), ('150', 'lobster_rolls'), ('40', 'hot_dogs'), ('164', 'boneless_wings'), ('245', 'crabs'), ('141', 'fishes'), ('272', 'pigs'), ('140', 'crab_legs'), ('292', 'snow_crab_legs'), ('245', 'crabs'), ('292', 'king_crab_legs'), ('292', 'lobsters'), ('293', 'fresh_oysters'), ('293', 'raw_oysters'), ('293', 'snails'), ('141', 'fishes'), ('308', 'nigiri_sushi')]\n",
      "121\n",
      "[('141', 'tail'), ('141', 'tail')]\n",
      "122\n",
      "[('12', 'lighter'), ('142', 'fresher'), ('208', 'faster')]\n",
      "123\n",
      "[('18', 'skin'), ('18', 'skin'), ('134', 'yolk'), ('18', 'skin'), ('18', 'skin'), ('18', 'skin'), ('18', 'skin')]\n",
      "124\n",
      "[('151', 'roe'), ('151', 'roe'), ('151', 'roe')]\n",
      "125\n",
      "[('1', 'green_peppers'), ('1', 'green_peppers'), ('1', 'green_peppers'), ('1', 'green_peppers'), ('1', 'green_peppers'), ('1', 'green_peppers'), ('1', 'green_peppers')]\n",
      "126\n",
      "[('53', 'delux'), ('53', 'burgr'), ('53', 'in_n_out_burger'), ('53', 'johnny_rockets'), ('116', 'jimmy_johns'), ('123', 'smash_burger'), ('156', 'pizza_hut'), ('156', 'pomo'), ('156', 'pizza_pizza'), ('156', 'subway'), ('156', 'grub_hub'), ('159', 'charity'), ('173', 'caesars'), ('173', 'ceasars'), ('186', 'insomnia'), ('186', 'dunkin_donuts'), ('186', 'dunkin'), ('156', 'subway')]\n",
      "127\n",
      "[('53', 'stadium'), ('121', 'mall'), ('156', 'theater'), ('173', 'arcade'), ('365', 'lounge'), ('365', 'pool')]\n",
      "128\n",
      "[('92', 'topping'), ('109', 'spread'), ('109', 'spread'), ('150', 'garnish'), ('156', 'base'), ('156', 'base'), ('156', 'base'), ('92', 'topping'), ('342', 'pairing'), ('342', 'dip'), ('92', 'topping')]\n",
      "129\n",
      "[('6', 'alfredo'), ('7', 'stir_fry'), ('8', 'burro'), ('43', 'blt'), ('59', 'benedict'), ('76', 'chimichanga'), ('77', 'fillet'), ('77', 'skewer'), ('79', 'risotto'), ('80', 'au_gratin'), ('97', 'bao'), ('106', 'tamale'), ('109', 'pate'), ('110', 'schnitzel'), ('112', 'chicken_wrap'), ('115', 'kebab'), ('97', 'bao'), ('146', 'tartar'), ('150', 'slider'), ('152', 'pita'), ('157', 'carbonara'), ('79', 'risotto'), ('152', 'pita'), ('159', 'flatbread'), ('159', 'flat_bread'), ('162', 'parmigiana'), ('162', 'ragu'), ('188', 'hash'), ('59', 'benedict'), ('188', 'frittata'), ('7', 'stir_fry'), ('218', 'chowder'), ('220', 'dumpling'), ('152', 'pita'), ('231', 'porridge'), ('76', 'chimichanga'), ('7', 'stir_fry'), ('266', 'casserole'), ('266', 'casserole'), ('293', 'rockefeller'), ('218', 'chowder'), ('294', 'ravioli'), ('295', 'scampi'), ('79', 'risotto'), ('294', 'ravioli'), ('342', 'fondue'), ('79', 'risotto'), ('294', 'ravioli')]\n",
      "130\n",
      "[('5', 'gorgonzola'), ('5', 'ricotta'), ('5', 'gorgonzola'), ('5', 'gorgonzola'), ('5', 'ricotta'), ('5', 'gorgonzola'), ('5', 'ricotta'), ('5', 'ricotta'), ('5', 'gorgonzola'), ('5', 'ricotta'), ('344', 'mascarpone'), ('5', 'ricotta')]\n",
      "131\n",
      "[('159', 'di')]\n",
      "132\n",
      "[('1', 'red_peppers'), ('36', 'roasted_tomatoes'), ('1', 'red_peppers'), ('1', 'red_peppers'), ('200', 'cranberries'), ('1', 'red_peppers'), ('1', 'red_peppers'), ('1', 'red_peppers'), ('315', 'candied_walnuts'), ('200', 'cranberries'), ('36', 'roasted_tomatoes')]\n",
      "133\n",
      "[('116', 'asiago'), ('116', 'asiago'), ('116', 'asiago'), ('116', 'asiago'), ('269', 'andouille'), ('116', 'asiago')]\n",
      "134\n",
      "[('40', 'hotdog'), ('40', 'cheesesteak'), ('40', 'coney'), ('40', 'corn_dog'), ('41', 'big_mac'), ('55', 'pig'), ('56', 'bagel'), ('109', 'hoagie'), ('139', 'shoe'), ('158', 'gyro'), ('159', 'pasty'), ('161', 'hot_dog'), ('56', 'bagel'), ('186', 'massage'), ('211', 'dog'), ('348', 'birthday_cake'), ('353', 'donut'), ('353', 'doughnut')]\n",
      "135\n",
      "[('110', 'cordon_bleu'), ('162', 'marsala'), ('162', 'marsala')]\n",
      "136\n",
      "[('162', 'fired')]\n",
      "137\n",
      "[('96', 'grass'), ('162', 'pan'), ('187', 'roast'), ('208', 'flame'), ('220', 'clay_pot'), ('253', 'wok'), ('270', 'ground'), ('162', 'pan'), ('162', 'pan')]\n",
      "138\n",
      "[('78', 'managers'), ('162', 'brothers'), ('208', 'fans')]\n",
      "139\n",
      "[('163', 'sun'), ('211', 'mist'), ('163', 'sun')]\n",
      "140\n",
      "[('165', 'rocks')]\n",
      "141\n",
      "[('0', 'vodka'), ('13', 'malt'), ('48', 'coke'), ('78', 'merlot'), ('80', 'red_wine'), ('0', 'vodka'), ('186', 'nitro'), ('187', 'kombucha'), ('191', 'champagne'), ('0', 'vodka'), ('356', 'espresso'), ('363', 'rum'), ('363', 'bourbon'), ('191', 'champagne')]\n",
      "142\n",
      "[('107', 'generous_portions'), ('167', 'fresh_ingredients'), ('167', 'quality_ingredients'), ('107', 'generous_portions')]\n",
      "143\n",
      "[('14', 'jalape_os'), ('14', 'jalapenos'), ('14', 'jalape_os'), ('22', 'hot_peppers'), ('24', 'breadcrumbs'), ('14', 'jalapenos'), ('14', 'jalape_os'), ('29', 'jalepenos'), ('14', 'jalapenos'), ('14', 'jalape_os'), ('29', 'jalepenos'), ('14', 'jalapenos'), ('14', 'jalape_os'), ('65', 'chili_peppers'), ('22', 'hot_peppers'), ('22', 'hot_peppers'), ('160', 'anchovies'), ('14', 'jalape_os'), ('22', 'hot_peppers'), ('14', 'jalape_os'), ('14', 'jalapenos'), ('254', 'pineapples'), ('14', 'jalape_os'), ('14', 'jalapenos'), ('315', 'pumpkin_seeds'), ('160', 'anchovies'), ('326', 'blue_cheese_crumbles')]\n",
      "144\n",
      "[('122', 'cleveland'), ('130', 'ny'), ('172', 'parma'), ('283', 'china')]\n",
      "145\n",
      "[('1', 'celery'), ('5', 'parsley'), ('66', 'cilantro'), ('70', 'cumin'), ('66', 'cilantro'), ('66', 'cilantro'), ('66', 'cilantro'), ('5', 'parsley'), ('66', 'cilantro'), ('1', 'celery'), ('1', 'celery'), ('1', 'celery'), ('5', 'parsley'), ('270', 'coriander'), ('66', 'cilantro'), ('70', 'cumin'), ('66', 'cilantro'), ('5', 'parsley'), ('1', 'celery')]\n",
      "146\n",
      "[('1', 'carrots'), ('246', 'snow_peas')]\n",
      "147\n",
      "[('178', 'machine'), ('184', 'station'), ('184', 'station'), ('178', 'machine'), ('372', 'card')]\n",
      "148\n",
      "[('63', 'fried_tofu'), ('63', 'panang'), ('64', 'noodle'), ('64', 'udon'), ('65', 'shoyu'), ('65', 'rice_cake'), ('72', 'crispy_rice'), ('74', 'cauliflower'), ('74', 'veg'), ('97', 'raw_beef'), ('97', 'chicken_feet'), ('100', 'roasted_pork'), ('102', 'goat'), ('102', 'marinated_chicken'), ('74', 'cauliflower'), ('103', 'polenta'), ('123', 'tempeh'), ('124', 'bbq_pork'), ('143', 'eel'), ('143', 'yam'), ('151', 'kimchi'), ('72', 'crispy_rice'), ('178', 'tofu'), ('64', 'udon'), ('219', 'soba'), ('219', 'chasu'), ('72', 'crispy_rice'), ('151', 'kimchi'), ('63', 'panang'), ('74', 'veg'), ('231', 'steamed_egg'), ('63', 'fried_tofu'), ('246', 'fish_fillet'), ('63', 'fried_tofu'), ('74', 'cauliflower'), ('100', 'roasted_pork'), ('263', 'grilled_pork'), ('97', 'chicken_feet'), ('74', 'cauliflower'), ('287', 'kofta'), ('293', 'shellfish'), ('298', 'poultry'), ('74', 'veg'), ('151', 'kimchi'), ('306', 'bulgogi'), ('124', 'bbq_pork'), ('311', 'orzo'), ('103', 'polenta'), ('361', 'curry'), ('178', 'tofu')]\n",
      "149\n",
      "[('53', 'angry'), ('78', 'rare'), ('180', 'stupid'), ('182', 'automatic'), ('182', 'awkward')]\n",
      "150\n",
      "[('180', 'gratuity')]\n",
      "151\n",
      "[('180', 'forever')]\n",
      "152\n",
      "[('180', 'nerve')]\n",
      "153\n",
      "[('182', 'stingy')]\n",
      "154\n",
      "[('2', 'citrus'), ('7', 'black_pepper'), ('11', 'tamarind'), ('11', 'tamarind'), ('63', 'lemongrass'), ('63', 'peanut'), ('65', 'soy'), ('66', 'pineapple'), ('7', 'black_pepper'), ('70', 'vinegar_based'), ('75', 'honey'), ('63', 'lemongrass'), ('2', 'citrus'), ('151', 'yuzu'), ('65', 'soy'), ('182', 'ginger'), ('183', 'lemon'), ('183', 'lemon'), ('206', 'mango'), ('182', 'ginger'), ('63', 'lemongrass'), ('63', 'peanut'), ('7', 'black_pepper'), ('254', 'root'), ('254', 'root'), ('7', 'black_pepper'), ('65', 'soy'), ('11', 'tamarind'), ('279', 'orange'), ('287', 'saffron'), ('287', 'saffron'), ('183', 'lemon'), ('182', 'ginger'), ('279', 'orange'), ('2', 'citrus'), ('182', 'ginger'), ('206', 'mango'), ('183', 'lemon'), ('66', 'pineapple'), ('279', 'orange'), ('75', 'honey')]\n",
      "155\n",
      "[('182', 'splurge')]\n",
      "156\n",
      "[('182', 'pop')]\n",
      "157\n",
      "[('182', 'higher')]\n",
      "158\n",
      "[('182', 'n')]\n",
      "159\n",
      "[('184', 'gas')]\n",
      "160\n",
      "[('154', 'extra_sauce'), ('178', 'sweetener'), ('185', 'hot_water'), ('178', 'sweetener'), ('203', 'refills'), ('185', 'hot_water'), ('185', 'hot_water'), ('222', 'hot_tea')]\n",
      "161\n",
      "[('3', 'jack'), ('41', 'heinz'), ('46', 'chuck'), ('66', 'serrano'), ('78', 'chris'), ('143', 'saba'), ('159', 'rosa'), ('172', 'nova'), ('186', 'coco'), ('219', 'don')]\n",
      "162\n",
      "[('186', 'plug')]\n",
      "163\n",
      "[('189', 'tart'), ('263', 'porky'), ('274', 'mild'), ('189', 'tart'), ('189', 'tart')]\n",
      "164\n",
      "[('189', 'sweet_tooth')]\n",
      "165\n",
      "[('26', 'curd'), ('189', 'mousse'), ('189', 'mousse'), ('189', 'mousse'), ('340', 'mouse'), ('344', 'torte'), ('344', 'ganache')]\n",
      "166\n",
      "[('2', 'berry'), ('190', 'matcha'), ('190', 'jasmine'), ('193', 'belgian'), ('195', 'peppermint'), ('206', 'peach'), ('190', 'jasmine'), ('212', 'green_tea'), ('225', 'taro'), ('225', 'taro'), ('190', 'matcha'), ('360', 'earl_grey'), ('360', 'lychee'), ('360', 'lavender'), ('2', 'berry'), ('206', 'peach'), ('363', 'passion_fruit'), ('363', 'blackberry'), ('367', 'ube'), ('212', 'green_tea'), ('225', 'taro')]\n",
      "167\n",
      "[('33', 'pecans'), ('33', 'almonds'), ('134', 'apples'), ('191', 'berries'), ('191', 'strawberries'), ('191', 'bananas'), ('242', 'cherries'), ('33', 'almonds'), ('315', 'grapes'), ('33', 'almonds'), ('134', 'apples'), ('33', 'pecans'), ('315', 'pistachios'), ('191', 'berries'), ('191', 'strawberries'), ('328', 'blueberries'), ('191', 'strawberries'), ('191', 'bananas'), ('191', 'berries'), ('242', 'cherries'), ('33', 'pecans'), ('346', 'raspberries'), ('33', 'almonds'), ('134', 'apples')]\n",
      "168\n",
      "[('187', 'grounds'), ('194', 'outdoor_seating'), ('194', 'chill_vibe'), ('211', 'air_conditioning'), ('211', 'misters'), ('211', 'outdoors')]\n",
      "169\n",
      "[('195', 'blend'), ('195', 'blend')]\n",
      "170\n",
      "[('195', 'fog')]\n",
      "171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('101', 'lettuce'), ('101', 'lettuce'), ('101', 'lettuce'), ('101', 'lettuce'), ('101', 'lettuce'), ('230', 'romaine_lettuce'), ('230', 'romaine_lettuce'), ('101', 'lettuce'), ('230', 'romaine_lettuce'), ('311', 'romaine'), ('101', 'lettuce')]\n",
      "172\n",
      "[('64', 'wonton'), ('201', 'lentil'), ('64', 'wonton'), ('201', 'lentil')]\n",
      "173\n",
      "[('72', 'msg'), ('204', 'grease'), ('204', 'grease'), ('237', 'dust'), ('72', 'msg'), ('204', 'grease'), ('275', 'crumbs')]\n",
      "174\n",
      "[('13', 'sticks'), ('18', 'rings'), ('13', 'sticks')]\n",
      "175\n",
      "[('15', 'wraps'), ('17', 'dips'), ('34', 'benedicts'), ('45', 'grilled_cheese_sandwiches'), ('114', 'bowls'), ('15', 'wraps'), ('17', 'dips'), ('148', 'bento_boxes'), ('181', 'dressings'), ('114', 'bowls'), ('186', 'iced_coffees'), ('192', 'crepes'), ('194', 'snacks'), ('17', 'dips'), ('207', 'sandwiches'), ('207', 'salads'), ('221', 'platters'), ('271', 'burritos'), ('302', 'courses'), ('181', 'dressings'), ('192', 'crepes'), ('207', 'salads'), ('207', 'sandwiches'), ('347', 'mains'), ('350', 'pastries'), ('362', 'baked_goods')]\n",
      "176\n",
      "[('208', 'cooled')]\n",
      "177\n",
      "[('73', 'marinated'), ('73', 'marinated'), ('90', 'marbled'), ('90', 'marbled'), ('73', 'marinated'), ('168', 'sauced'), ('208', 'spiced'), ('211', 'brewed')]\n",
      "178\n",
      "[('208', 'crave')]\n",
      "179\n",
      "[('39', 'patty'), ('92', 'crust'), ('39', 'patty'), ('123', 'brioche_bun'), ('129', 'burger_patty'), ('138', 'dough'), ('159', 'pizza_crust'), ('138', 'dough'), ('210', 'bun'), ('228', 'tortilla'), ('231', 'wrapper')]\n",
      "180\n",
      "[('211', 'refreshing'), ('339', 'decadent'), ('211', 'refreshing'), ('367', 'creative')]\n",
      "181\n",
      "[('90', 'shit'), ('115', 'togo'), ('139', 'crack'), ('194', 'art'), ('211', 'crap'), ('339', 'heaven'), ('211', 'crap')]\n",
      "182\n",
      "[('211', 'wind')]\n",
      "183\n",
      "[('178', 'milk'), ('185', 'almond_milk'), ('212', 'ice'), ('178', 'milk'), ('241', 'sparkling'), ('212', 'ice'), ('212', 'ice'), ('178', 'milk')]\n",
      "184\n",
      "[('139', 'bites'), ('199', 'loaves'), ('216', 'scoops'), ('216', 'scoops'), ('216', 'scoops')]\n",
      "185\n",
      "[('90', 'fork'), ('118', 'container'), ('218', 'spoon'), ('218', 'ladle'), ('355', 'bag')]\n",
      "186\n",
      "[('218', 'brim')]\n",
      "187\n",
      "[('218', 'wtf')]\n",
      "188\n",
      "[('135', 'soaked'), ('135', 'soaked'), ('237', 'swimming'), ('135', 'soaked')]\n",
      "189\n",
      "[('65', 'sour'), ('65', 'sour'), ('65', 'sour'), ('65', 'sour')]\n",
      "190\n",
      "[('17', 'queso'), ('37', 'cornbread'), ('45', 'bacon_jam'), ('62', 'naan'), ('62', 'raita'), ('65', 'edamame'), ('81', 'mac_cheese'), ('86', 'guacamole'), ('86', 'guacamole'), ('106', 'asada'), ('81', 'mac_cheese'), ('37', 'cornbread'), ('159', 'cheesy_bread'), ('172', 'mortadella'), ('187', 'brewed_coffee'), ('62', 'naan'), ('204', 'bruchetta'), ('221', 'miso_soup'), ('65', 'edamame'), ('224', 'hummus'), ('231', 'purple_rice'), ('62', 'naan'), ('231', 'purple_rice'), ('65', 'edamame'), ('263', 'japchae'), ('265', 'pancit'), ('265', 'general_tao_chicken'), ('263', 'japchae'), ('65', 'edamame'), ('293', 'gazpacho'), ('302', 'tuna_tartar'), ('303', 'grilled_squid'), ('65', 'edamame'), ('332', 'couscous'), ('362', 'oatmeal'), ('366', 'caviar'), ('366', 'bone_marrow')]\n",
      "191\n",
      "[('4', 'drunken_noodles'), ('7', 'tom_yum'), ('8', 'posole'), ('62', 'roti'), ('63', 'kung_pao_chicken'), ('63', 'laksa'), ('63', 'gyoza'), ('63', 'chow_mein'), ('66', 'ceviche'), ('72', 'shoyu_ramen'), ('74', 'chicken_soup'), ('100', 'chow_fun'), ('100', 'won_ton_soup'), ('103', 'lasagna'), ('146', 'tartare'), ('151', 'takoyaki'), ('103', 'lasagna'), ('162', 'lasagne'), ('74', 'chicken_soup'), ('217', 'orange_chicken'), ('219', 'tonkotsu'), ('63', 'laksa'), ('220', 'pho_broth'), ('7', 'tom_yum'), ('226', 'pad_thai'), ('228', 'tortilla_soup'), ('244', 'larb'), ('74', 'chicken_soup'), ('63', 'chow_mein'), ('265', 'chow_mien'), ('66', 'ceviche'), ('265', 'chow_mien'), ('283', 'yellow_curry'), ('283', 'yakisoba'), ('283', 'sinigang'), ('283', 'pad_see_ew'), ('292', 'etouffee'), ('103', 'lasagna'), ('308', 'sashimi'), ('309', 'shumai'), ('217', 'orange_chicken'), ('100', 'chow_fun'), ('63', 'kung_pao_chicken'), ('309', 'beef_chow_fun'), ('4', 'drunken_noodles'), ('310', 'panang_curry')]\n",
      "192\n",
      "[('139', 'ass'), ('227', 'belly'), ('227', 'belly'), ('272', 'shoulder')]\n",
      "193\n",
      "[('1', 'carrots')]\n",
      "194\n",
      "[('72', 'cabbage'), ('72', 'ground_pork'), ('74', 'broccoli'), ('80', 'asparagus'), ('74', 'broccoli'), ('74', 'broccoli'), ('100', 'bok_choy'), ('100', 'daikon'), ('80', 'asparagus'), ('126', 'mixed_greens'), ('80', 'asparagus'), ('72', 'cabbage'), ('152', 'radish'), ('74', 'broccoli'), ('126', 'mixed_greens'), ('72', 'cabbage'), ('100', 'bok_choy'), ('74', 'broccoli'), ('72', 'cabbage'), ('74', 'broccoli'), ('256', 'white_beans'), ('72', 'cabbage'), ('100', 'bok_choy'), ('152', 'radish'), ('100', 'daikon'), ('272', 'red_cabbage'), ('72', 'cabbage'), ('100', 'bok_choy'), ('100', 'daikon'), ('126', 'mixed_greens'), ('272', 'red_cabbage'), ('80', 'asparagus'), ('80', 'asparagus')]\n",
      "195\n",
      "[('21', 'artichoke_dip'), ('32', 'kielbasa'), ('41', 'texas_toast'), ('44', 'coleslaw'), ('44', 'cole_slaw'), ('44', 'potato_salad'), ('78', 'creamed_corn'), ('80', 'saut_ed_spinach'), ('86', 'salsa'), ('91', 'toast'), ('44', 'coleslaw'), ('44', 'coleslaw'), ('44', 'potato_salad'), ('44', 'coleslaw'), ('21', 'artichoke_dip'), ('86', 'salsa'), ('91', 'toast'), ('188', 'sourdough_toast'), ('221', 'veggie_tempura'), ('44', 'coleslaw'), ('256', 'tabbouleh'), ('32', 'kielbasa'), ('44', 'potato_salad'), ('44', 'cole_slaw'), ('91', 'toast')]\n",
      "196\n",
      "[('131', 'small_pieces'), ('131', 'small_pieces'), ('233', 'tiny_pieces')]\n",
      "197\n",
      "[('234', 'lover')]\n",
      "198\n",
      "[('72', 'bean'), ('72', 'bean'), ('72', 'bean'), ('72', 'bean'), ('72', 'bean'), ('72', 'bean'), ('72', 'bean')]\n",
      "199\n",
      "[('12', 'circles'), ('12', 'circles')]\n",
      "200\n",
      "[('7', 'leaf'), ('123', 'grain'), ('123', 'grain'), ('123', 'grain'), ('7', 'leaf'), ('7', 'leaf'), ('237', 'flower')]\n",
      "201\n",
      "[('1', 'carrots'), ('246', 'snow_peas'), ('1', 'carrots')]\n",
      "202\n",
      "[('19', 'breast')]\n",
      "203\n",
      "[('9', 'chandeliers'), ('9', 'chandeliers'), ('236', 'lights'), ('236', 'ceilings'), ('236', 'murals')]\n",
      "204\n",
      "[('9', 'tablecloths'), ('236', 'linens'), ('236', 'pillows')]\n",
      "205\n",
      "[('237', 'ivy')]\n",
      "206\n",
      "[('5', 'mozzarella'), ('5', 'pancetta'), ('22', 'mozz'), ('5', 'pancetta'), ('22', 'fresh_mozzarella'), ('28', 'salami'), ('5', 'pancetta'), ('28', 'salami'), ('47', 'caramelized_onion'), ('5', 'mozzarella'), ('5', 'mozzarella'), ('28', 'salami'), ('5', 'mozzarella'), ('22', 'fresh_mozzarella'), ('22', 'mozz'), ('28', 'salami'), ('28', 'salami'), ('22', 'mozz'), ('5', 'mozzarella'), ('22', 'fresh_mozzarella'), ('5', 'mozzarella'), ('311', 'field_greens'), ('22', 'fresh_mozzarella'), ('5', 'mozzarella'), ('5', 'pancetta')]\n",
      "207\n",
      "[('239', 'tattoos')]\n",
      "208\n",
      "[('218', 'ball'), ('218', 'ball')]\n",
      "209\n",
      "[('2', 'drizzle'), ('200', 'smear'), ('2', 'drizzle')]\n",
      "210\n",
      "[('2', 'coconut_cream'), ('19', 'sea_salt'), ('66', 'lime'), ('66', 'lime'), ('174', 'pistachio'), ('174', 'pistachio'), ('189', 'dark_chocolate'), ('189', 'cocoa'), ('189', 'amaretto'), ('19', 'sea_salt'), ('193', 'cinnamon'), ('193', 'brown_sugar'), ('195', 'cardamom'), ('19', 'sea_salt'), ('193', 'cinnamon'), ('66', 'lime'), ('242', 'mint'), ('66', 'lime'), ('66', 'lime'), ('193', 'cinnamon'), ('295', 'white_wine'), ('337', 'nutella'), ('342', 'chestnut'), ('189', 'dark_chocolate'), ('174', 'pistachio'), ('344', 'dulce_de_leche'), ('189', 'amaretto'), ('189', 'cocoa'), ('348', 'marshmallow'), ('348', 'cereal'), ('193', 'cinnamon')]\n",
      "211\n",
      "[('244', 'rangoon'), ('244', 'rangoon')]\n",
      "212\n",
      "[('68', 'shrimps'), ('78', 'filets'), ('78', 'filets'), ('68', 'shrimps'), ('245', 'crab_claws'), ('68', 'shrimps'), ('68', 'shrimps'), ('245', 'crab_claws')]\n",
      "213\n",
      "[('1', 'carrots')]\n",
      "214\n",
      "[('19', 'breast')]\n",
      "215\n",
      "[('1', 'cucumber'), ('2', 'green_bean'), ('52', 'quinoa'), ('1', 'cucumber'), ('82', 'tomato'), ('1', 'cucumber'), ('1', 'cucumber'), ('1', 'cucumber'), ('82', 'tomato'), ('82', 'tomato'), ('52', 'quinoa'), ('1', 'cucumber'), ('1', 'cucumber'), ('1', 'cucumber'), ('82', 'tomato'), ('315', 'pear'), ('52', 'quinoa'), ('315', 'pear')]\n",
      "216\n",
      "[('253', 'chestnuts')]\n",
      "217\n",
      "[('7', 'peas'), ('7', 'peas'), ('7', 'peas'), ('7', 'peas'), ('7', 'peas'), ('7', 'peas'), ('7', 'peas')]\n",
      "218\n",
      "[('1', 'bell_pepper'), ('1', 'bell_pepper'), ('1', 'bell_pepper'), ('263', 'fungus'), ('1', 'bell_pepper')]\n",
      "219\n",
      "[('1', 'big_chunks'), ('12', 'large_chunks'), ('131', 'chunks'), ('131', 'chunks'), ('1', 'big_chunks'), ('12', 'large_chunks'), ('131', 'chunks'), ('131', 'chunks'), ('131', 'chunks'), ('131', 'chunks'), ('131', 'chunks')]\n",
      "220\n",
      "[('96', 'bones'), ('128', 'thighs'), ('245', 'legs'), ('96', 'bones'), ('96', 'bones'), ('263', 'intestines'), ('96', 'bones')]\n",
      "221\n",
      "[('100', 'cutlet'), ('100', 'cutlet'), ('100', 'cutlet')]\n",
      "222\n",
      "[('19', 'breast')]\n",
      "223\n",
      "[('1', 'carrots')]\n",
      "224\n",
      "[('269', 'smoky_flavor'), ('269', 'smokey_flavor')]\n",
      "225\n",
      "[('76', 'enchilada'), ('76', 'enchilada'), ('76', 'enchilada'), ('76', 'enchilada'), ('76', 'enchilada')]\n",
      "226\n",
      "[('10', 'sour_cream'), ('10', 'sour_cream'), ('24', 'pico'), ('28', 'fig_jam'), ('10', 'sour_cream'), ('10', 'sour_cream'), ('24', 'pico'), ('10', 'sour_cream'), ('10', 'sour_cream'), ('24', 'pico'), ('228', 'pico_de_gallo'), ('10', 'sour_cream'), ('10', 'sour_cream')]\n",
      "227\n",
      "[('162', 'absolutely_horrible'), ('248', 'yum_yum'), ('273', 'meh')]\n",
      "228\n",
      "[('47', 'smothered'), ('135', 'coated'), ('47', 'smothered'), ('47', 'smothered')]\n",
      "229\n",
      "[('19', 'breast')]\n",
      "230\n",
      "[('8', 'tortilla_chips'), ('13', 'sweet_potato_tots'), ('13', 'fried_mushrooms'), ('20', 'kettle_chips'), ('21', 'cheese_sticks'), ('22', 'curds'), ('30', 'tots'), ('30', 'tots'), ('41', 'regular_fries'), ('41', 'smash_fries'), ('41', 'skinny_fries'), ('41', 'potato_chips'), ('41', 'shoestring_fries'), ('22', 'curds'), ('41', 'potato_wedges'), ('41', 'crinkle_cut_fries'), ('20', 'kettle_chips'), ('13', 'fried_mushrooms'), ('41', 'homemade_chips'), ('52', 'frites'), ('59', 'breakfast_potatoes'), ('52', 'frites'), ('88', 'baked_potatoes'), ('112', 'curly_fries'), ('30', 'tots'), ('20', 'kettle_chips'), ('41', 'potato_wedges'), ('41', 'potato_chips'), ('129', 'onion_rings'), ('41', 'potato_chips'), ('8', 'tortilla_chips'), ('162', 'bread_sticks'), ('21', 'cheese_sticks'), ('164', 'mozzarella_sticks'), ('164', 'chicken_fingers'), ('129', 'onion_rings'), ('164', 'pretzels'), ('59', 'breakfast_potatoes'), ('20', 'kettle_chips'), ('41', 'potato_chips'), ('208', 'chicken_wings'), ('217', 'egg_rolls'), ('208', 'chicken_wings'), ('208', 'chicken_wings'), ('269', 'pickled_vegetables'), ('278', 'french_fries'), ('217', 'egg_rolls')]\n",
      "231\n",
      "[('135', 'shell'), ('135', 'shell'), ('135', 'shell'), ('135', 'shell')]\n",
      "232\n",
      "[('13', 'garlic_parm'), ('17', 'nacho_cheese'), ('20', 'thousand_island'), ('41', 'honey_mustard'), ('20', 'thousand_island'), ('17', 'nacho_cheese'), ('63', 'thai_basil'), ('41', 'honey_mustard'), ('13', 'garlic_parm'), ('41', 'honey_mustard'), ('128', 'charcoal'), ('151', 'wasabi'), ('151', 'wasabi'), ('63', 'thai_basil'), ('63', 'thai_basil'), ('151', 'wasabi'), ('151', 'wasabi'), ('151', 'wasabi')]\n",
      "233\n",
      "[('1', 'carrots')]\n",
      "234\n",
      "[('295', 'ink')]\n",
      "235\n",
      "[('71', 'types'), ('90', 'cuts'), ('90', 'cuts'), ('90', 'cuts'), ('90', 'cuts')]\n",
      "236\n",
      "[('239', 'moon'), ('302', 'fin')]\n",
      "237\n",
      "[('63', 'teriyaki'), ('102', 'adobo'), ('63', 'teriyaki'), ('63', 'teriyaki'), ('63', 'teriyaki'), ('102', 'adobo'), ('63', 'teriyaki')]\n",
      "238\n",
      "[('1', 'carrots')]\n",
      "239\n",
      "[('2', 'lump_crab'), ('5', 'squash'), ('8', 'ground_beef'), ('10', 'cheddar'), ('16', 'feta'), ('22', 'blue_cheese'), ('22', 'pepper_jack'), ('22', 'havarti'), ('16', 'feta'), ('30', 'bleu_cheese'), ('22', 'havarti'), ('5', 'squash'), ('41', 'pimento_cheese'), ('10', 'cheddar'), ('30', 'bleu_cheese'), ('45', 'gruyere'), ('22', 'pepper_jack'), ('8', 'ground_beef'), ('48', 'chorizo'), ('51', 'pepperoni'), ('66', 'shredded_chicken'), ('69', 'sausage'), ('69', 'zucchini'), ('77', 'grilled_shrimp'), ('80', 'mashed_potato'), ('30', 'bleu_cheese'), ('69', 'sausage'), ('97', 'sliced_beef'), ('102', 'shredded_pork'), ('69', 'zucchini'), ('51', 'pepperoni'), ('10', 'cheddar'), ('22', 'havarti'), ('41', 'pimento_cheese'), ('45', 'gruyere'), ('123', 'avacado'), ('123', 'bologna'), ('126', 'goat_cheese'), ('22', 'blue_cheese'), ('77', 'grilled_shrimp'), ('141', 'crabmeat'), ('8', 'ground_beef'), ('51', 'pepperoni'), ('160', 'prosciutto'), ('16', 'feta'), ('22', 'blue_cheese'), ('30', 'bleu_cheese'), ('172', 'italian_sausage'), ('172', 'spicy_sausage'), ('16', 'feta'), ('178', 'egg'), ('179', 'avocado'), ('69', 'zucchini'), ('10', 'cheddar'), ('10', 'cheddar'), ('45', 'gruyere'), ('172', 'spicy_sausage'), ('16', 'feta'), ('209', 'onion'), ('209', 'spinach'), ('77', 'grilled_shrimp'), ('66', 'shredded_chicken'), ('123', 'avacado'), ('230', 'spring_mix'), ('22', 'blue_cheese'), ('30', 'bleu_cheese'), ('2', 'lump_crab'), ('230', 'spring_mix'), ('254', 'watercress'), ('5', 'squash'), ('8', 'ground_beef'), ('5', 'squash'), ('10', 'cheddar'), ('141', 'crabmeat'), ('209', 'onion'), ('179', 'avocado'), ('16', 'feta'), ('230', 'spring_mix'), ('209', 'spinach'), ('126', 'goat_cheese'), ('160', 'prosciutto'), ('22', 'blue_cheese'), ('30', 'bleu_cheese'), ('5', 'squash'), ('209', 'spinach'), ('209', 'onion'), ('126', 'goat_cheese')]\n",
      "240\n",
      "[('16', 'tzatziki'), ('20', 'dill'), ('20', 'dill'), ('80', 'chimichurri'), ('16', 'tzatziki'), ('20', 'dill'), ('20', 'dill'), ('182', 'tahini'), ('20', 'dill'), ('182', 'tahini'), ('182', 'tahini'), ('20', 'dill'), ('340', 'brown_butter')]\n",
      "241\n",
      "[('317', 'kalamata')]\n",
      "242\n",
      "[('238', 'roma'), ('238', 'roma')]\n",
      "243\n",
      "[('63', 'stir_fried'), ('65', 'pickled'), ('69', 'sauteed'), ('72', 'minced'), ('80', 'yukon'), ('65', 'pickled'), ('102', 'stewed'), ('109', 'thinly_sliced'), ('123', 'oven_roasted'), ('126', 'chopped'), ('130', 'pounded'), ('131', 'shredded'), ('230', 'iceberg'), ('131', 'shredded'), ('246', 'boiled'), ('246', 'boiled'), ('109', 'thinly_sliced'), ('102', 'stewed'), ('289', 'saut_ed'), ('72', 'minced'), ('289', 'saut_ed'), ('63', 'stir_fried'), ('230', 'iceberg'), ('131', 'shredded'), ('363', 'candied')]\n",
      "244\n",
      "[('320', 'situation')]\n",
      "245\n",
      "[('290', 'leaves'), ('290', 'leaves')]\n",
      "246\n",
      "[('19', 'breast')]\n",
      "247\n",
      "[('328', 'carrot'), ('340', 'lava'), ('342', 'black_forest'), ('328', 'carrot')]\n",
      "248\n",
      "[('7', 'pea'), ('7', 'pea'), ('7', 'pea'), ('7', 'pea')]\n",
      "249\n",
      "[('6', 'alfredo_sauce'), ('13', 'bbq_sauce'), ('21', 'ranch_dressing'), ('39', 'secret_sauce'), ('47', 'au_jus'), ('63', 'peanut_sauce'), ('70', 'mustard_sauce'), ('80', 'peppercorn_sauce'), ('91', 'hollandaise'), ('91', 'hollandaise_sauce'), ('13', 'bbq_sauce'), ('13', 'bbq_sauce'), ('112', 'barbecue_sauce'), ('21', 'ranch_dressing'), ('150', 'tartar_sauce'), ('150', 'cocktail_sauce'), ('151', 'spicy_sauce'), ('158', 'pita_bread'), ('162', 'tomato_sauce'), ('6', 'alfredo_sauce'), ('21', 'ranch_dressing'), ('182', 'hot_sauce'), ('47', 'au_jus'), ('63', 'peanut_sauce'), ('13', 'bbq_sauce'), ('221', 'ginger_dressing'), ('151', 'spicy_sauce'), ('230', 'salad_dressing'), ('162', 'tomato_sauce'), ('150', 'cocktail_sauce'), ('158', 'pita_bread'), ('162', 'tomato_sauce'), ('151', 'spicy_sauce'), ('47', 'au_jus'), ('13', 'bbq_sauce'), ('151', 'spicy_sauce'), ('63', 'peanut_sauce'), ('311', 'dressing'), ('311', 'house_dressing'), ('311', 'balsamic_dressing'), ('311', 'italian_dressing'), ('162', 'tomato_sauce'), ('21', 'ranch_dressing'), ('333', 'marinara_sauce'), ('311', 'dressing')]\n",
      "250\n",
      "[('336', 'br')]\n",
      "251\n",
      "[('66', 'y'), ('182', 'o'), ('336', 'l'), ('336', 'e')]\n",
      "252\n",
      "[('8', 'chimi'), ('43', 'sandwhich'), ('59', 'omelette'), ('63', 'noodle_dish'), ('84', 'shake'), ('43', 'sandwhich'), ('188', 'scramble'), ('190', 'slush'), ('192', 'pancake'), ('63', 'noodle_dish'), ('192', 'pancake'), ('228', 'skillet'), ('192', 'pancake'), ('192', 'pancake'), ('192', 'pancake'), ('303', 'sampler'), ('337', 'crepe'), ('340', 'souffle'), ('340', 'trio'), ('342', 'platter'), ('348', 'brownie'), ('348', 'cookie'), ('348', 'sundae'), ('348', 'cone'), ('348', 'waffle_cone'), ('192', 'pancake'), ('350', 'cupcake'), ('350', 'macaroon'), ('352', 'roll'), ('362', 'pastry'), ('367', 'pie'), ('84', 'shake')]\n",
      "253\n",
      "[('339', 'goodness')]\n",
      "254\n",
      "[('37', 'bread_pudding'), ('41', 'fry_sauce'), ('127', 'cinnamon_roll'), ('159', 'bianca'), ('165', 'sangria'), ('166', 'quiche'), ('186', 'cold_brew_coffee'), ('188', 'avocado_toast'), ('190', 'lemonade'), ('127', 'cinnamon_roll'), ('195', 'biscotti'), ('165', 'sangria'), ('340', 'cheesecake'), ('340', 'creme_brulee'), ('340', 'chocolate_cake'), ('37', 'bread_pudding'), ('340', 'tiramisu'), ('340', 'creme_brule'), ('340', 'flourless_chocolate_cake'), ('340', 'panna_cotta'), ('340', 'key_lime_pie'), ('340', 'flan'), ('340', 'chocolate_souffle'), ('340', 'lemon_tart'), ('340', 'profiteroles'), ('340', 'sticky_toffee_pudding'), ('340', 'tres_leches_cake'), ('340', 'napoleon'), ('340', 'tres_leches'), ('342', 'apple_tart'), ('342', 'rice_pudding'), ('342', 'apple_pie'), ('342', 'gulab_jamun'), ('344', 'chocolate_brownie'), ('344', 'strawberry_shortcake'), ('344', 'banana_cream_pie'), ('344', 'red_velvet_cake'), ('344', 'coconut_cream_pie'), ('344', 'pecan_pie'), ('344', 'chocolate_chip_cookie'), ('349', 'french_toast'), ('350', 'gelato'), ('350', 'cannoli'), ('350', 'baklava'), ('195', 'biscotti'), ('356', 'hot_chocolate'), ('360', 'affogato'), ('360', 'chai'), ('166', 'quiche'), ('367', 'honey_toast'), ('367', 'banana_pudding'), ('367', 'funnel_cake'), ('375', 'shaved_ice')]\n",
      "255\n",
      "[('18', 'perfectly_crispy'), ('39', 'beefy'), ('47', 'charred'), ('88', 'crisp'), ('47', 'charred'), ('134', 'buttery'), ('47', 'charred'), ('149', 'firm'), ('159', 'cheesy'), ('201', 'tangy'), ('201', 'tangy'), ('134', 'buttery'), ('269', 'smokey'), ('201', 'tangy'), ('134', 'buttery'), ('289', 'slightly_spicy'), ('301', 'succulent'), ('311', 'zesty'), ('315', 'ripe'), ('88', 'crisp'), ('341', 'crunchy'), ('134', 'buttery')]\n",
      "256\n",
      "[('342', 'rush')]\n",
      "257\n",
      "[('340', 'fried_banana'), ('344', 'salted_caramel'), ('344', 'key_lime'), ('344', 'red_velvet'), ('344', 'mint_chocolate_chip'), ('344', 'pumpkin_pie'), ('348', 'cookie_dough'), ('348', 'churro'), ('348', 'rocky_road'), ('367', 'smores'), ('367', 'cotton_candy'), ('367', 'peanut_butter_cup'), ('367', 'oreo_cookie')]\n",
      "258\n",
      "[('67', 'overpowering'), ('277', 'overly_sweet'), ('303', 'overly_salty'), ('277', 'overly_sweet')]\n",
      "259\n",
      "[('45', 'buttermilk'), ('50', 'oreo'), ('178', 'caramel'), ('183', 'coconut'), ('189', 'almond'), ('189', 'hazelnut'), ('189', 'toffee'), ('189', 'gingerbread'), ('193', 'blueberry'), ('193', 'chocolate_chip'), ('205', 'peanut_butter'), ('189', 'toffee'), ('206', 'strawberry'), ('242', 'white_chocolate'), ('266', 'pumpkin'), ('277', 'apple'), ('205', 'peanut_butter'), ('315', 'cherry'), ('178', 'caramel'), ('242', 'white_chocolate'), ('189', 'toffee'), ('189', 'almond'), ('189', 'hazelnut'), ('344', 'butterscotch'), ('266', 'pumpkin'), ('344', 'carmel'), ('344', 'graham_cracker'), ('344', 'chantilly'), ('346', 'banana'), ('348', 'vanilla'), ('205', 'peanut_butter'), ('50', 'oreo'), ('350', 'chocolate_dipped'), ('183', 'coconut'), ('193', 'chocolate_chip'), ('206', 'strawberry'), ('363', 'raspberry'), ('277', 'apple'), ('193', 'blueberry'), ('315', 'cherry'), ('363', 'maple'), ('363', 'rhubarb'), ('363', 'apricot'), ('367', 'turtle')]\n",
      "260\n",
      "[('47', 'perfectly_seasoned'), ('62', 'perfectly_spiced'), ('47', 'perfectly_seasoned'), ('80', 'perfectly_prepared'), ('88', 'prepared_perfectly'), ('47', 'perfectly_seasoned'), ('88', 'seasoned_nicely'), ('88', 'super_tasty'), ('47', 'perfectly_seasoned'), ('107', 'seasoned_perfectly'), ('109', 'perfectly_toasted'), ('47', 'perfectly_seasoned'), ('149', 'nicely_seasoned'), ('149', 'perfectly_fried'), ('150', 'lightly_fried'), ('151', 'amazingly_fresh'), ('218', 'steaming_hot'), ('107', 'seasoned_perfectly'), ('295', 'al_dente'), ('339', 'heavenly'), ('347', 'perfectly_cooked')]\n",
      "261\n",
      "[('2', 'jelly'), ('11', 'powder'), ('18', 'beer_batter'), ('135', 'glaze'), ('11', 'powder'), ('186', 'foam'), ('189', 'pudding'), ('190', 'soft_serve'), ('199', 'syrup'), ('201', 'yogurt'), ('225', 'tapioca'), ('2', 'jelly'), ('2', 'jelly'), ('269', 'bark'), ('199', 'syrup'), ('340', 'sponge_cake'), ('342', 'jello'), ('344', 'sherbet'), ('344', 'meringue'), ('344', 'icing'), ('344', 'frosting'), ('135', 'glaze'), ('344', 'buttercream'), ('201', 'yogurt'), ('348', 'custard'), ('190', 'soft_serve'), ('348', 'fudge'), ('356', 'mocha'), ('189', 'pudding'), ('360', 'sorbet'), ('2', 'jelly'), ('199', 'syrup'), ('375', 'mochi'), ('225', 'tapioca')]\n",
      "262\n",
      "[('127', 'croissants'), ('174', 'brownies'), ('189', 'tarts'), ('190', 'milk_teas'), ('192', 'muffins'), ('174', 'brownies'), ('281', 'cakes'), ('350', 'cookies'), ('281', 'cakes'), ('174', 'brownies'), ('189', 'tarts'), ('350', 'eclairs'), ('350', 'truffles'), ('350', 'cream_puffs'), ('192', 'muffins')]\n",
      "263\n",
      "[('49', 'steakhouse'), ('53', 'shack'), ('60', 'brewery'), ('87', 'pub'), ('49', 'steakhouse'), ('187', 'culture'), ('292', 'seafood_section'), ('293', 'oyster_bar'), ('318', 'market'), ('322', 'bistro'), ('342', 'caf'), ('350', 'bakery'), ('367', 'chain')]\n",
      "264\n",
      "[('53', 'plain_jane'), ('90', 'premium'), ('182', 'optional'), ('182', 'specific'), ('187', 'fair_trade'), ('187', 'organic'), ('188', 'english'), ('298', 'authentic_chinese'), ('338', 'basic'), ('351', 'vegan'), ('351', 'gluten_free'), ('363', 'seasonal')]\n",
      "265\n",
      "[('62', 'spice_levels'), ('125', 'products'), ('127', 'bagels'), ('144', 'margaritas'), ('155', 'ovens'), ('127', 'bagels'), ('169', 'wines'), ('182', 'checks'), ('188', 'breakfast_burritos'), ('127', 'bagels'), ('205', 'chocolate_chip_cookies'), ('237', 'flowers'), ('348', 'cones'), ('350', 'treats'), ('205', 'chocolate_chip_cookies'), ('353', 'donuts'), ('353', 'doughnuts'), ('360', 'flavours'), ('373', 'foods')]\n",
      "266\n",
      "[('358', 'joke')]\n",
      "267\n",
      "[('182', 'containers'), ('190', 'shots'), ('203', 'baskets'), ('182', 'containers'), ('218', 'spoons'), ('241', 'bottles'), ('359', 'cups')]\n",
      "268\n",
      "[('364', 'forgot')]\n",
      "269\n",
      "[('4', 'scallop'), ('5', 'olive'), ('6', 'penne'), ('39', 'oxtail'), ('41', 'pretzel'), ('41', 'garlic_parmesan'), ('41', 'fried_green_tomato'), ('45', 'portobello'), ('46', 'bison'), ('46', 'elk'), ('52', 'ahi_tuna'), ('62', 'thai_curry'), ('62', 'saag'), ('65', 'miso'), ('66', 'diablo'), ('68', 'crab'), ('4', 'scallop'), ('46', 'bison'), ('100', 'vege'), ('109', 'ham_cheese'), ('41', 'pretzel'), ('45', 'portobello'), ('130', 'kobe'), ('68', 'crab'), ('143', 'ahi'), ('143', 'dynamite'), ('144', 'barbacoa'), ('65', 'miso'), ('5', 'olive'), ('65', 'miso'), ('41', 'pretzel'), ('65', 'miso'), ('39', 'oxtail'), ('5', 'olive'), ('240', 'veal'), ('247', 'creole'), ('100', 'vege'), ('299', 'prawn'), ('301', 'short_rib'), ('4', 'scallop'), ('65', 'miso'), ('302', 'diver'), ('52', 'ahi_tuna'), ('143', 'dynamite'), ('5', 'olive'), ('45', 'portobello'), ('347', 'duck'), ('351', 'veggie'), ('366', 'lobster'), ('4', 'scallop')]\n",
      "270\n",
      "[('350', 'woman'), ('367', 'kid')]\n",
      "271\n",
      "[('367', 'boy')]\n",
      "272\n",
      "[('85', 'comped'), ('85', 'comped')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 13, 21, 29, 39, 61, 77, 94, 108, 128, 141, 154, 176, 185, 196, 206, 215, 235, 244, 251, 263, 273, 282, 292, 311, 335, 377]\n",
      "red\n",
      "['bell_pepper', 'chilies', 'green_peppers', 'carrots', 'jalapeno', 'red_peppers', 'fresh_basil', 'cucumber', 'red_onion', 'green_onions', 'cashews', 'big_chunks', 'celery', 'scallions', 'cucumbers']\n",
      "['olives', 'roasted_garlic', 'oregano', 'squash', 'artichokes', 'olive', 'mozzarella', 'gorgonzola', 'parsley', 'ricotta', 'pancetta']\n",
      "['mole', 'burro', 'posole', 'tamales', 'ground_beef', 'chimi', 'tortilla_chips']\n",
      "['tomatillo', 'chiles', 'tamarind', 'lime_juice', 'sprinkled', 'chilli', 'limes', 'peanuts', 'powder', 'chili_powder', 'chili_flakes', 'rim', 'shredded_cheese']\n",
      "\n",
      "fries\n",
      "['pickles', 'kettle_chips', 'thousand_island', 'dill', 'vinegar', 'mayonnaise', 'relish', 'strings', 'american_cheese', 'spuds', 'kraut', 'banana_peppers', 'dijon_mustard']\n",
      "\n",
      "cheese\n",
      "['caramelized_onions', 'blue_cheese', 'jalape_os', 'kraut', 'pepper_jack', 'gooey_cheese', 'pickles', 'mozz', 'carmelized_onions', 'pancetta', 'hot_peppers', 'sweet_peppers', 'gruyere_cheese', 'parmesan_cheese', 'sauteed_mushrooms', 'fresh_mozzarella', 'popper', 'curds', 'havarti', 'tomato_jam']\n",
      "['green_peppers', 'processed_cheese', 'breadcrumbs', 'diced_tomatoes', 'feta', 'grilled_vegetables', 'jack_cheese', 'pico', 'flavor_profile', 'crumbles']\n",
      "\n",
      "bacon\n",
      "['cheddar_cheese', 'caramelized_onions', 'applewood_smoked_bacon', 'smoked_bacon', 'mustard', 'pepper_jack_cheese', 'havarti', 'melted_cheese', 'pimento', 'dill', 'kielbasa', 'saut_ed_mushrooms', 'multigrain']\n",
      "\n",
      "burger\n",
      "['hotdog', 'cheesesteak', 'hot_dogs', 'buns', 'coney', 'corn_dog', 'sonoran']\n",
      "['tots', 'regular_fries', 'smash_fries', 'animal_style', 'onion_straws', 'big_mac', 'skinny_fries', 'potato_chips', 'pickle', 'shoestring', 'shoestring_fries', 'curds', 'honey_mustard', 'pimento_cheese', 'loaded_fries', 'fry_sauce', 'tenders', 'fires', 'strips', 'burger_patties', 'pretzel', 'potato_wedges', 'thousand_island', 'texas_toast', 'crinkle_cut_fries', 'nuggets', 'strings', 'fried_mushrooms', 'homemade_chips', 'barbecue', 'garlic_parmesan', 'naked', 'reg', 'heinz', 'fried_green_tomato']\n",
      "['caramelized_onions', 'cheddar', 'fried_egg', 'bleu_cheese', 'carmelized_onions', 'cheddar_cheese', 'smash', 'american_cheese', 'swiss', 'swiss_cheese', 'applewood_smoked_bacon', 'smoked_bacon', 'crispy_bacon', 'bacon_jam', 'brie', 'gruyere_cheese', 'portobello', 'grilled_cheese_sandwiches', 'rocket', 'popper', 'cowboy', 'gruyere', 'maple_syrup', 'gouda', 'sauteed_onions', 'smoked_gouda', 'earl', 'shrooms', 'piled_high', 'buttermilk', 'tomato_jam', 'stack', 'dijon']\n",
      "['aioli', 'pepper_jack', 'charred', 'hamburger_patty', 'fried_onions', 'wheat_bun', 'smothered', 'char', 'fixings', 'dripping', 'smashed', 'saut_ed_onions', 'ground_beef', 'thousand_island_dressing', 'wedges', 'au_jus', 'sauteed_mushrooms', 'mayonnaise', 'toasted_bun', 'perfectly_seasoned', 'runny', 'cooked_correctly', 'caramelized_onion']\n",
      "['pepper_jack_cheese', 'jalapenos', 'jalape_os', 'coke', 'jalapeno', 'condiments', 'relish', 'jalepenos', 'chorizo', 'sriracha', 'sour_cream', 'nacho_cheese', 'green_chiles', 'hatch', 'lb', 'fully_loaded']\n",
      "['oreo', 'frozen_custard']\n",
      "['parmesan', 'provolone', 'mozzarella', 'gorgonzola', 'pepperoni']\n",
      "['truffle', 'frites', 'brussel_sprouts', 'brussels_sprouts', 'ahi_tuna', 'quinoa']\n",
      "['pastrami', 'mustard', 'corned_beef', 'smoked_meat', 'kraut', 'sauerkraut', 'pig']\n",
      "\n",
      "spicy\n",
      "['spices', 'tamarind', 'spice_levels', 'vindaloo', 'thai_curry', 'perfectly_spiced', 'naan', 'lamb_skewers', 'jerk', 'roti', 'hakka', 'pakoras', 'saag', 'raita']\n",
      "['peanut_sauce', 'teriyaki', 'stir_fried', 'thai_basil', 'noodle_dish', 'lemongrass', 'fried_tofu', 'panang', 'dumplings', 'papaya', 'kung_pao_chicken', 'peanut', 'wontons', 'laksa', 'gyoza', 'sticky_rice', 'chow_mein', 'rice_noodles', 'potstickers', 'lettuce_wraps', 'soft_shell_crab']\n",
      "['sour', 'sriracha', 'soy_sauce', 'miso', 'chili_peppers', 'chili_paste', 'shoyu', 'sesame_oil', 'pickled', 'cucumber', 'sesame', 'edamame', 'rice_cake', 'scallions', 'chili_flakes', 'soy', 'hoisin_sauce', 'seaweed']\n",
      "['cilantro', 'chile', 'chilies', 'pineapple', 'burn', 'lime', 'red_chili', 'tomatillo', 'chiles', 'diablo', 'y', 'serrano', 'shredded_chicken', 'ceviche', 'lime_juice', 'chilis']\n",
      "['basil', 'sausage', 'hot_peppers', 'roasted_garlic', 'red_peppers', 'sauteed', 'pesto', 'zucchini', 'cucumbers']\n",
      "['mayo', 'vinegar', 'cumin', 'slaw', 'herb', 'relish', 'mustard_sauce', 'mustard', 'perfectly_seasoned', 'mayonnaise', 'vinegar_based', 'flavor_profile']\n",
      "['white_rice', 'cauliflower', 'coconut_milk', 'brown_sauce', 'green_beans', 'broccoli', 'veg', 'chicken_soup', 'sizzling_plate', 'mixed_vegetables', 'tilapia']\n",
      "\n",
      "steak\n",
      "['sirloin', 'tenderloin', 'flank_steak', 'fillet', 'medallions', 'marinated', 'grilled_shrimp', 'breast', 'mahi_mahi', 'pork_tenderloin', 'skewer']\n",
      "['asparagus', 'mushrooms', 'chimichurri', 'fingerling_potatoes', 'mash_potatoes', 'peppercorn_sauce', 'saut_ed_spinach', 'seared', 'whipped_potatoes', 'grilled_asparagus', 'potatoes_au_gratin', 'mashed_potato', 'roasted_potatoes', 'foie_gras', 'brussel_sprouts', 'frites', 'saut_ed_mushrooms', 'perfectly_prepared', 'sweet_potato', 'smashed_potatoes', 'sauteed_mushrooms', 'au_gratin', 'wild_mushrooms', 'yukon', 'grilled_salmon', 'peppercorn', 'red_wine', 'potato_puree', 'scalloped_potatoes', 'sauteed_spinach', 'brussels']\n",
      "\n",
      "beef\n",
      "['shredded_pork', 'bbq_chicken', 'breast', 'goat', 'stewed', 'ground_meat', 'ham', 'marinated_chicken', 'green_peppers', 'fresh_vegetables', 'adobo']\n",
      "\n",
      "sandwich\n",
      "['lettuce', 'goat_cheese', 'greens', 'balsamic', 'croutons', 'chopped', 'feta_cheese', 'cucumber', 'mixed_greens', 'vinaigrette']\n",
      "\n",
      "cut\n",
      "\n",
      "fish\n",
      "['tilapia', 'fishes', 'perch', 'flounder', 'bass', 'grilled_shrimp', 'tail', 'fried_oysters', 'filets', 'sole', 'jumbo_shrimp', 'crabmeat', 'ribeye', 'pan_seared', 'green_beans', 'seabass', 'cornmeal']\n",
      "['red_snapper', 'ahi', 'eel', 'soft_shell_crab', 'yellow_tail', 'saba', 'seared_tuna', 'yam', 'spicy_tuna', 'dynamite', 'sake', 'salmon_belly', 'poki']\n",
      "['wasabi', 'seaweed', 'miso', 'spicy_sauce', 'spicy_mayo', 'kimchi', 'yuzu', 'skewers', 'roe', 'amazingly_fresh', 'ponzu_sauce', 'nori', 'takoyaki', 'crispy_rice']\n",
      "\n",
      "pizza\n",
      "['pesto', 'prosciutto', 'olives', 'basil', 'artichoke', 'gorgonzola', 'olive_oil', 'artichokes', 'feta', 'burrata', 'anchovies', 'parmesan_cheese', 'roasted_garlic', 'feta_cheese', 'roasted_red_peppers', 'fennel', 'balsamic_vinegar', 'red_peppers', 'asiago', 'anchovy', 'roasted_peppers', 'hearts']\n",
      "\n",
      "extra\n",
      "\n",
      "coffee\n",
      "['smoothie', 'matcha', 'jasmine', 'lemonade', 'boba_tea', 'slush', 'soft_serve', 'milk_teas', 'shots']\n",
      "\n",
      "bread\n",
      "\n",
      "hot\n",
      "\n",
      "bowl\n",
      "['udon', 'miso', 'soba', 'sate', 'tonkotsu', 'don', 'chashu', 'chasu', 'bun_bo_hue', 'oxtail']\n",
      "\n",
      "white\n",
      "\n",
      "shrimp\n",
      "\n",
      "veggies\n",
      "['greens', 'romaine_lettuce', 'fresh_greens', 'spring_mix', 'roasted_veggies', 'watercress', 'pineapples', 'root']\n",
      "['grilled_veggies', 'chickpeas', 'peas', 'lentils', 'parsley', 'squash', 'cauliflower', 'bell_pepper', 'chick_peas', 'oils', 'seeds', 'cottage_cheese', 'tabbouleh', 'pita_bread', 'white_beans', 'naan', 'paste']\n",
      "\n",
      "pork\n",
      "\n",
      "dry\n",
      "\n",
      "rice\n",
      "\n",
      "seafood\n",
      "['king_crab', 'shellfish', 'shrimps', 'oyster', 'snow_crab_legs', 'mussel', 'crabs', 'lobsters', 'crab_cakes', 'fresh_oysters', 'raw_oysters', 'crab_claws', 'rockefeller', 'chowder', 'oyster_bar', 'crabmeat', 'maine', 'shell', 'shells', 'snow', 'seafood_section', 'smoked_salmon', 'fried_oysters', 'stone_crab', 'peel', 'jumbo', 'snails', 'alligator', 'gazpacho']\n",
      "['squid', 'ocean', 'wasabi', 'roe', 'miso', 'ponzu', 'spicy_sauce', 'marinade', 'nori']\n",
      "['sea_bass', 'fishes', 'snapper', 'cod', 'toro', 'tuna_tartar', 'quail', 'seared', 'mackerel', 'sea_urchin', 'kobe_beef', 'carpaccio', 'hamachi', 'butterfish', 'grilled_squid', 'diver', 'sea_scallops', 'freshness', 'ahi_tuna', 'courses', 'seared_tuna', 'fin']\n",
      "['crawfish', 'dungeness_crab', 'catfish', 'grits', 'etouffee', 'sampler', 'pan', 'sausages']\n",
      "['fish_balls', 'saut_ed', 'pork_chops', 'veg', 'snow_peas', 'carrots', 'bean_sprouts', 'poultry']\n",
      "['skewers', 'kimchi', 'bulgogi', 'galbi', 'fusion', 'overly_salty']\n",
      "['sashimi', 'imitation_crab', 'seaweed', 'spicy_tuna', 'teriyaki', 'edamame', 'dynamite', 'dragon_roll', 'spicy_salmon']\n",
      "['stir_fried', 'shumai', 'rangoon', 'steam', 'orange_chicken', 'potstickers', 'chow_fun', 'kung_pao_chicken', 'fried_fish', 'beef_chow_fun', 'bbq_pork', 'sticky_rice', 'steamed_rice', 'malaysian', 'firecracker', 'fried_dumplings', 'authentic_chinese']\n",
      "\n",
      "salad\n",
      "['lettuce', 'cucumber', 'cucumbers', 'carrots', 'cilantro']\n",
      "['squash', 'lentils', 'butternut_squash', 'peas', 'pea']\n",
      "['beets', 'pine_nuts', 'pear', 'beet', 'pears', 'walnuts', 'grapes', 'brie', 'almonds', 'candied_walnuts', 'tart', 'cranberries', 'pumpkin_seeds', 'candied_pecans', 'pecans', 'raisins']\n",
      "['lemon', 'ginger', 'orange']\n",
      "['quinoa', 'greens', 'spring_mix', 'avocado', 'couscous', 'crisp', 'shredded', 'fresh_greens', 'citrus', 'brown', 'market', 'roasted_vegetables', 'bistro', 'sprouts']\n",
      "['spinach', 'feta', 'olives', 'mozzarella', 'gorgonzola', 'kalamata', 'artichokes', 'parmesan', 'prosciutto', 'fennel', 'fresh_mozzarella', 'pancetta', 'roasted_red_peppers', 'basil_pesto', 'hearts', 'asiago', 'roasted_tomatoes', 'ricotta']\n",
      "['tomatoes', 'onions', 'mushrooms', 'red_onion', 'peppers', 'red_peppers', 'chickpeas', 'pepper', 'bell_pepper', 'bean_sprouts']\n",
      "['corn', 'black_beans']\n",
      "['blue_cheese', 'celery', 'wedge', 'dill', 'green_beans', 'breast', 'blue_cheese_crumbles', 'pickles', 'ranch_dressing', 'portobello', 'ranch', 'bleu_cheese', 'aioli', 'banana_peppers', 'brussels_sprouts']\n",
      "['balsamic', 'parsley', 'olive_oil', 'anchovies', 'herb', 'olive', 'oil', 'balsamic_vinegar', 'herbs', 'parmesan_cheese']\n",
      "['chili']\n",
      "\n",
      "dessert\n",
      "['cheesecake', 'creme_brulee', 'mousse', 'chocolate_cake', 'impressive', 'bread_pudding', 'tiramisu', 'creme_brule', 'flourless_chocolate_cake', 'panna_cotta', 'key_lime_pie', 'flan', 'lava', 'chocolate_souffle', 'lemon_tart', 'profiteroles', 'budino', 'polenta', 'champagne', 'affogato', 'fondue', 'tres_leches_cake', 'brown_butter', 'mains', 'sharing', 'napoleon', 'tres_leches', 'fried_banana', 'rose', 'mouse']\n",
      "['caramel', 'pudding', 'dark_chocolate', 'pistachio', 'peanut_butter', 'salted_caramel', 'white_chocolate', 'toffee', 'almond', 'key_lime', 'chocolate_brownie', 'butterscotch', 'red_velvet', 'ganache', 'strawberry_shortcake', 'sherbet', 'meringue', 'pumpkin', 'red_velvet_cake', 'pecan', 'praline', 'coconut_cream_pie', 'dulce_de_leche', 'fudge', 'pecan_pie', 'overly_sweet', 'jelly', 'mascarpone', 'strudel', 'red_velvet_cupcake', 'nut', 'rocky_road', 'glaze', 'buttercream', 'carmel', 'chestnut', 'mint_chocolate_chip', 'graham_cracker', 'amaretto', 'pumpkin_pie', 'cocoa', 'chantilly']\n",
      "['banana', 'strawberries', 'carrot', 'spinach', 'bananas', 'fresh_fruit', 'berries', 'fruits', 'cherries', 'pecans', 'raspberries', 'maple_syrup', 'almonds', 'compote']\n",
      "['french_toast', 'toast', 'pancakes', 'pancake']\n",
      "['gelato', 'cookies', 'cakes', 'brownies', 'tarts', 'crepes', 'pastries', 'cannoli', 'macaroon', 'treats', 'bakery', 'eclairs', 'candy', 'truffles', 'chocolate_chip_cookies', 'woman', 'biscotti']\n",
      "['donuts', 'donut', 'doughnut', 'doughnuts']\n",
      "['hot_chocolate', 'latte', 'cappuccino', 'mocha', 'chai']\n",
      "['soda', 'cups', 'beverages']\n",
      "['pastry', 'chocolate_chip', 'caf', 'baked_goods', 'muffin', 'muffins', 'danish', 'scone', 'oatmeal', 'raisin', 'quiche']\n",
      "['strawberry', 'sorbet', 'raspberry', 'lemon', 'apple', 'tart', 'blueberry', 'berry', 'cinnamon', 'peach', 'ginger', 'seasonal', 'pear', 'martini', 'rhubarb', 'pineapple', 'candied', 'blackberry', 'apricot', 'orange', 'lavender', 'honey', 'pairing', 'bourbon', 'refreshing', 'fig']\n",
      "['brownie', 'cookie', 'nutella', 'flat', 'sundae', 'oreo', 'smores', 'waffle', 'cotton_candy', 'brownie_sundae', 'center', 'milkshake', 'churro', 'banana_pudding', 'peanut_butter_cup', 'chain', 'cones', 'scoops', 'oreo_cookie', 'funnel_cake', 'turtle', 'sweet_potato', 'boy']\n",
      "['vanilla', 'crepe', 'whipped_cream', 'topping', 'scoop', 'syrup', 'marshmallow', 'cone', 'icing', 'birthday_cake', 'bag', 'cereal', 'ube', 'hot_fudge', 'whip_cream', 'sprinkles', 'frozen_yogurt', 'chocolate_chips', 'chocolate_syrup', 'waffle_cone', 'powdered_sugar']\n",
      "['flavours', 'rice_pudding', 'rush', 'yogurt', 'baklava', 'jello', 'sticky_rice', 'japanese', 'platter', 'gulab_jamun', 'assorted', 'foods', 'curry', 'tofu', 'balls']\n",
      "['black', 'corn', 'bean']\n",
      "['ice', 'milk', 'green_tea', 'mochi', 'soft_serve', 'matcha', 'shaved_ice', 'sesame', 'lychee', 'taro', 'tapioca']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:310: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n",
      "/home/jiaxinh3/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/cluster/bicluster.py:312: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  for c in range(self.n_clusters))\n"
     ]
    }
   ],
   "source": [
    "entity_ratio_alltopics1 = {}\n",
    "entity_count_alltopics1 = {}\n",
    "\n",
    "for test_topic in topic_hier1['ROOT']:\n",
    "    if test_topic in topic_hier['ROOT']:\n",
    "        entity_ratio_alltopics1[test_topic] = entity_ratio_alltopics[test_topic]\n",
    "        entity_count_alltopics1[test_topic] = entity_count_alltopics[test_topic]\n",
    "        continue\n",
    "    \n",
    "    sim_ranking = topic_sim(test_topic, vocabulary_inv, topic_emb, word_emb)\n",
    "    cap_ranking, target_cap = rank_cap_customed(word_cap, vocabulary_inv, [vocabulary[word] for word in topic_hier[train_topic]])\n",
    "    coefficient = max(word_cap[test_topic] / word_cap[train_topic],1)\n",
    "    test_cand = aggregate_ranking(sim_ranking, cap_ranking, word_cap, test_topic, vocabulary_inv, pretrain, target_cap*coefficient)\n",
    "    print(f'test topic: {test_topic}')  \n",
    "    test_data = process_test_data([test_topic], test_cand, max_seq_length)\n",
    "    print(f\"test data point number: {len(test_data)}\")\n",
    "    \n",
    "#     if len(test_data) > 10000:\n",
    "#         test_data=test_data[:10000]\n",
    "\n",
    "    entity_ratio, entity_count = relation_inference(test_data, TEST_BATCH_SIZE)\n",
    "    entity_ratio_alltopics1[test_topic] = entity_ratio\n",
    "    entity_count_alltopics1[test_topic] = entity_count\n",
    "    \n",
    "    \n",
    "child_entities_count1 = sum_all_rel(topic_hier1['ROOT'], entity_count_alltopics1, mode='child')\n",
    "\n",
    "child_entities1 = type_consistent(child_entities_count1, ename2embed_bert)\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "clusters_all = {}\n",
    "k=0\n",
    "start_list = [0]\n",
    "for j,topic in enumerate(topic_hier1['ROOT']):   \n",
    "    \n",
    "    X = []\n",
    "    for ent in child_entities1[topic]:\n",
    "        if ent not in word_emb:\n",
    "            continue\n",
    "        X.append(word_emb[ent])\n",
    "    X = np.array(X)\n",
    "    if len(X) == 0:\n",
    "        print(topic)\n",
    "        continue\n",
    "\n",
    "    clustering = AffinityPropagation().fit(X)\n",
    "    n_clusters = max(clustering.labels_) + 1\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        clusters[str(i)] = [child_entities1[topic][x] for x in range(len(clustering.labels_)) if clustering.labels_[x] == i]\n",
    "        \n",
    "        clusters_all[str(k)] = clusters[str(i)]\n",
    "        k+=1\n",
    "    start_list.append(k)\n",
    "print('-----')\n",
    "print(k)\n",
    "print('-----')\n",
    "#     new_clusters = type_consistent_col(clusters, ename2embed_bert)\n",
    "    \n",
    "#     i=0\n",
    "#     for k in new_clusters: \n",
    "# #         print(clusters[k])\n",
    "#         if len(new_clusters[k])>1:\n",
    "#             print(i,':', end='\\t')\n",
    "#             print(new_clusters[k])\n",
    "#             i += 1\n",
    "# print(clusters_all)    \n",
    "# new_clusters = type_consistent_col(clusters_all, ename2embed_bert)\n",
    "new_clusters = type_consistent_cocluster(clusters_all, ename2embed_bert, n_cluster_min = 2, print_cls = True, save_file='dblp_field+_cls8')\n",
    "\n",
    "print(start_list)\n",
    "\n",
    "tmp = defaultdict(list)\n",
    "\n",
    "topic_idx = 0\n",
    "for k in range(len(clusters_all)):\n",
    "    if k >= start_list[topic_idx]:\n",
    "#         print('\\n',topic_hier1['ROOT'][topic_idx])\n",
    "        topic_idx += 1\n",
    "    if str(k) in new_clusters and len(new_clusters[str(k)]) > 1:\n",
    "#         print(new_clusters[str(k)])\n",
    "        tmp[topic_hier1['ROOT'][topic_idx-1]].append(new_clusters[str(k)])\n",
    "\n",
    "child_entities1 = tmp\n",
    "for t in topic_hier['ROOT']:\n",
    "    child_entities1[t] = child_entities[t]\n",
    "    \n",
    "for t in topic_hier1['ROOT']:\n",
    "    print(t)\n",
    "    for l in child_entities1[t]:\n",
    "        print(l)\n",
    "    print('')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the keyword taxonomy, nodes in which will be enriched later by concept learning.\n",
    "with open(os.path.join(dataset, 'keyword_taxonomy.txt'), 'w') as fout:\n",
    "    for topic in topic_hier1['ROOT']:  \n",
    "        if len(child_entities1[topic]) > 0:      \n",
    "            fout.write(topic+'\\n')\n",
    "            for cls in child_entities1[topic]:\n",
    "                fout.write(' '.join(cls)+'\\n')\n",
    "            fout.write('\\n')\n",
    "\n",
    "for topic in topic_hier1['ROOT']:\n",
    "    if len(child_entities1[topic]) > 0:\n",
    "        with open(os.path.join(dataset, 'topics_'+topic+'.txt'),'w') as fout:\n",
    "            for cls in child_entities1[topic]:\n",
    "                fout.write(' '.join(cls)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
